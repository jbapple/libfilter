% VLDB journal - https://www.springer.com/journal/778/submission-guidelines Fee for open access $2780 https://www.springer.com/journal/778/open-access-publishing#Fees%20and%20Funding
% JEA - journal https://dl.acm.org/journal/jea/instructions-for-authors Fee for open access $1700 or $1300 https://www.acm.org/publications/openaccess#h-open-access-pricing
% TKDE - Journal https://www.computer.org/csdl/journal/tk can't figure out if open access available with fee
% Software: Practice and Experience

% NSDI - September 9, https://www.usenix.org/conference/nsdi22, April 4-6, Renton, WA
% SIGMOD - September 15th, feedback November 1-8, accept reject December 6th https://2022.sigmod.org/calls_papers_important_dates.shtml, Philly, June 12-17th
% EDBT - https://conferences.inf.ed.ac.uk/edbticdt2022/?contents=important_dates.html, Edinburgh, October 1 deadline, 29 March 2022
% VLDB -  February 28th, https://vldb.org/2022/ Sydney, September 5-9

% SODA - Passed, no 2023 date yet https://www.siam.org/conferences/cm/conference/soda22
% USENIX ATC - July 11, 2022 - July 13, 2022 | Carlsbad, CA, no 2022 website yet https://www.usenix.org/conferences/byname/131
% KDD - Passed, no 2022 website yet https://kdd.org/conferences
% ICDE - Passed, no 2023 website yet https://icde2022.ieeecomputer.my/research-track/
% SEA -Passed, no 2022 date yet
% ALENEX - Passed, no 2023 website yet
% ESA - Passed, no 2022 website yet, Berlin/Potsdam http://esa-symposium.org/
% ICDE - Passed, no 2023 website yet

% TODS
% SIGIR
% CIKM

% IEEE big data

% PVLDB - same conference as VLDB, submit here first http://vldb.org/pvldb/vol15-submission/
% CIDR - August 27, 2021: Submission deadline for all contributions, October 15, 2021: author notification for all contributions, Chaminade, on-site, 6 pages maximum

\newif\ifanon
\anontrue

\newif\ifepigraph
\epigraphfalse

% \documentclass[letterpaper]{article}
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}
%\documentclass[sigconf]{acmart}
%\documentclass[manuscript,screen,review]{acmart}

%% \setcopyright{acmcopyright}
%% \copyrightyear{2018}
%% \acmYear{2018}
%% \acmDOI{10.1145/1122445.1122456}

%% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%   June 03--05, 2018, Woodstock, NY}
%% \acmPrice{15.00}
%% \acmISBN{978-1-4503-XXXX-X/18/06}

\pdfoutput=1

\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{graphicx}
%\PassOptionsToPackage{hyphens}{url}\usepackage[pdftitle={Stretching your data with taffy filters}]
%\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{microtype}
% TODO: fix underscores
%\usepackage[strings]{underscore}
%\usepackage{doi}
%\usepackage{nicefrac}
\usepackage{listings}
%\usepackage{todonotes}
\usepackage{epigraph}
%\usepackage{ifthen}
%% \usepackage{tikz}
%% \usetikzlibrary{arrows.meta}
%\usepackage[export]{adjustbox}
%\usepackage{framed}
\usepackage{float}
\usepackage{caption}


\newtheorem{theorem}{Theorem}

\lstset{
%    frame=tb, % draw a frame at the top and bottom of the code block
%    tabsize=2, % tab space width
%    showstringspaces=false, % don't mark spaces in strings
  numbers=left, % display line numbers on the left
%    commentstyle=\color{green}, % comment color
%    keywordstyle=\color{blue}, % keyword color
%    stringstyle=\color{red}, % string color
  basicstyle=\ttfamily,
  basewidth = {.48em},
  captionpos=b
}

\DeclareMathOperator{\adj}{adj}

%\renewcommand\UrlFont{\color{blue}\rmfamily}

%% \newcommand{\reals}{\mathbb{R}}
%% \newcommand{\rats}{\mathbb{Q}}
%% \newcommand{\nats}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
%% \newcommand{\cplx}{\mathbb{C}}
\newcommand{\defeq}{\;\genfrac{}{}{0pt}{3}{\text{def}}{=}\;}
\newcommand{\dotcup}{\ensuremath{\mathaccent\cdot\cup}}
\newcommand{\etal}{et al.}

%\pagestyle{empty}

% https://tex.stackexchange.com/questions/171803/change-font-size-of-the-verbatim-environment
% \newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}

%% \usepackage{etoolbox}
%% \makeatletter
%% \patchcmd{\@verbatim}
%%   {\verbatim@font}
%%   {\verbatim@font\small}
%%   {}{}
%% \makeatother


%% %% The following content must be adapted for the final version
%% % paper-specific
%% \newcommand\vldbdoi{XX.XX/XXX.XX}
%% \newcommand\vldbpages{XXX-XXX}
%% % issue-specific
%% \newcommand\vldbvolume{15}
%% \newcommand\vldbissue{1}
%% \newcommand\vldbyear{2022}
%% % should be fine as it is
%% \newcommand\vldbauthors{\authors}
%% \newcommand\vldbtitle{\shorttitle} 
%% % leave empty if no availability url should be set
%% \newcommand\vldbavailabilityurl{https://github.com/jbapple/libfilter}
%% % whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
%% \newcommand\vldbpagestyle{plain} 
\begin{document}

\title{Stretch Your Data With Taffy Filters}
\ifanon
\else
\author{Jim Apple}
% \orcid{0000-0002-8685-9451}
%% \affiliation{%
%%   \city{Los Gatos}
%%   \state{California}
%% %  \country{United States of America}
%% }
%\email{jbapple@jbapple.com}
\fi

%% \begin{CCSXML}
%% <ccs2012>
%%    <concept>
%%        <concept_id>10003752.10003809.10010055.10010056</concept_id>
%%        <concept_desc>Theory of computation~Bloom filters and hashing</concept_desc>
%%        <concept_significance>500</concept_significance>
%%    </concept>
%%  </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Theory of computation~Bloom filters and hashing}
%% % \ccsdesc[500]{Information systems~Point lookups}


% \keywords{Bloom filters, dictionaries, hash tables}

%\thispagestyle{empty}

\maketitle

\begin{abstract}
Popular approximate membership query structures such as Bloom filters and cuckoo filters are widely used in databases, security, and networking.
These structures support two operations -- insert and lookup; lookup always returns true on elements inserted into the structure, while it returns true with some probability $\varepsilon \ll 1$ on elements {\em not} inserted into the structure.
These latter elements are called false positives.
Compensatory for these false positives, filters can be much smaller than hash tables that represent the same set.
However, unlike hash tables, cuckoo filters and Bloom filters must be initialized with the intended number of inserts to be performed, and cannot grow larger --
inserts beyond this number may fail or increase the false positive probability.
This paper presents designs and implementations of filters than can grow without inserts failing and without significantly increasing the false positive probability, even if the filters are created with a small initial size.
The resulting code is available on GitHub under a permissive open source license.
\ifepigraph
\epigraph{If you can look into the seeds of time, and say which grain will grow and which will not, speak then unto me.}{Macbeth}
\fi
\end{abstract}



%% %%% do not modify the following VLDB block %%
%% %%% VLDB block start %%%
%% \pagestyle{\vldbpagestyle}
%% \begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
%% \vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
%% \href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
%% \endgroup
%% \begingroup
%% \renewcommand\thefootnote{}\footnote{\noindent
%% This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
%% \raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
%% ISSN 2150-8097. \\
%% \href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
%% }\addtocounter{footnote}{-1}\endgroup
%% %%% VLDB block end %%%

%% %%% do not modify the following VLDB block %%
%% %%% VLDB block start %%%
%% \ifdefempty{\vldbavailabilityurl}{}{
%% \vspace{.3cm}
%% \begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
%% The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
%% \endgroup
%% }
%% %%% VLDB block end %%%


\section{Introduction}

%% TODO: concrete applications: LSM trees, joins (BF union in Impala) (28, 31, 27, 12)

%% TODO: LSM trees in a single filter: no ribbon filters required, all data in one bloomier filter

%% TODO: TBF can't have satellite data, TCF can

%% TODO: huge pages

%% TODO: Vacuumize or Mortonize TCF and MTCF

The Bloom filter is a ubiquitous data structure that allows storing a set with a low amount of space.
Bloom filters support the operations insert -- which adds an item to the set -- and lookup, which returns true if an element is in the filter; if an element is not in the filter, true is returned with some configurable probability $\varepsilon \ll 1$.
This is called the ``false positive probability'', or ``fpp''

There are a number of other structures also supporting insert and lookup with a false positive probability greater than 0~\cite{vacuum,morton-journal,ribbon,xor-filter,quotient-filter,broom,vector-quotient}.
A lookup operation with these guarantees is sometimes called an ``approximate membership query'', and structures that support approximate membership queries are sometimes referred to ``AMQ structures'' or just ``filters''.
The significant interest in filters is reflective of their utility in applications such as databases, security, and networking~\cite{split-bloom, vacuum, quotient-filter, malware, profile-similarity, invertible, flooding-filter, summary-cache, prefix-matching-filter}.

%Genetics, search engines, blockchain, machine learning, storage~\cite{dna-filter, bitfunnel, bitcoin-filter, gene-search-filter, sequencing-filter, model-training-filter, deduplication-filter}

Each of the filter structures cited above supports approximate membership queries on sets with a given maximum size, but the question of extensible (or {\itshape extendable} or {\itshape incremental} or {\itshape growable}) filters that can increase in capacity as more elements are inserted is little studied.
The classic answer is to create a sequence of Bloom filters, possibly of increasing sizes and/or lower false positive probabilities~\cite{dynamic-bloom,scalable-bloom}.
Inserts occur on the last filter to be created and lookups must search each filter.
Even when this keeps the false positive rate low, lookup times balloon from constant to poly-logarithmic or even linear in $n$, the number of elements inserted~\cite{psw,logarithm,consistent-cuckoo}. %The Dynamic Cuckoo Filter
Additionally, the space usage grows as $\Omega(n \lg n)$, at which point a traditional hash table would do the same work in the same space with constant-time operations and an $n^{-c}$ false positive probability, where $c$ depends on the constant in $\Omega(n \lg n)$.
A newer approach to manage growing filters is to use cuckoo or quotient filters in which, when the filter grows, the false positive probability doubles~\cite{logarithm,morton-journal,vacuum,rsqf}.

%% TODO: implement the sequence-of-Bloom-filters approach and benchmark it.

Pagh \etal{} introduced structures that perform lookups in $O(\lg n)$ or $O(1)$ time while still allowing the structure to grow and keep a low false positive rate (not exceeding a threshold specified when the structure was created), all while using no more than $O(\lg \lg n + \lg (1/\varepsilon))$ bits of space per element~\cite{psw}.

Growable filters are potentially useful in situations where there is no known bound on the number of keys to be inserted.
One example is in joins in query processing systems.
It is often beneficial to performance to create and populate a filter for the build side of a join:
the filter, being much smaller than the full output of the build-side hash table construction, can be pushed-down to the probe side to reduce the number of rows that need to be tested against the build output~\cite{tpch-filter}.
If there are any predicates on the build side, or if the build side has incomplete or inaccurate distinct value count statistics, it is not possible to predict the eventual size of the filter.
Systems like Impala estimate the cardinality when initializing the filter and then discard the filter if the estimate was too low~\cite{impala}.
Using growable filters would allow these filters to continue to be populated and used in the probe side.

Furthermore, Impala is a distributed query engine, and populates a filter for each node that participates in creating the build side of a join, then merges these filters before distributing the merged filter to the nodes participating in the probe side of the join; ApproxJoin does the same~\cite{approxjoin, impala}.
%When using Cuckoo filters, this merge is impossible. When using Bloom filters,
With Bloom filters, this merge procedure requires that every filter be as large as the eventual merged filter, which is much larger than the size of the filter each build node would need on its own, wasting memory on build nodes.

Another example where growable filters are useful is in log-structured merge trees (``LSM trees'')~\cite{lsm}.
Log-structured merge trees store data in sorted ``runs'' of exponentially-increasing size.
In order to cheaply discover if a key is present in a run, systems like RocksDB equip each run with a filter~\cite{lsm, ribbon}.
In typical LSM trees, the data in each run is fixed for the lifetime of that level, so each filter can be created with knowledge of the number of keys it will contain, even when the structure as a whole can grow without bound.
However, point lookups that go through the filters require accessing $\lg n$ filters, where $n$ is the number of keys in the LSM tree.
A single growable filter structure can reduce this to a single filter query by storing one structure for all keys, rather than $\lg n$ structures.

%% TODO: associate satellite data with each entry. Explain in LSM subsection of applications section?

%% TODO: data deduplication?

Pagh \etal{} ends with an open problem of implementing these structures in a practical way~\cite{psw}.
This work answers that challenge with three practical extensible filters:

\begin{enumerate}
\item Section~\ref{pbf}  presents the {\em taffy block filter}           (``TBF''),   a Bloom-filter-backed  AMQ structure with $O(\lg n)$ lookup cost.
\item Section~\ref{tcf}  presents the {\em taffy cuckoo filter}          (``TCF''),   a cuckoo-hashing-based AMQ structure with $O(1)$ lookup cost.
\item Section~\ref{mtcf} presents the {\em minimal taffy cuckoo filters} (``MTCFs''), a cuckoo-hashing-based AMQ structure that decreases the space needed in a TCF by up to a factor of 2.
\end{enumerate}

Section~\ref{eval} describes experimental performance results on all three taffy filters and what circumstances each is suited for. %\footnote{Experiments can be replicated with \url{https://github.com/jbapple/libfilter}}
Section~\ref{conclusion} concludes with open questions.

% TODO: block filters with 16 hash functions - when useful?

\section{Prior work}

\subsection{Split block Bloom filters}

%% TODO: address space usage problem as $\varepsilon$ shrinks and $k$ increases well past 8.
%% For example, since ideal $k$ is $lg (1/\varepsilon)$, once $\varepsilon < 2^{-16}$, we should really double the block size.
%% In the Pagh \etal{} structure, this can happen when the target fpp is $2^{-8}$ but there are 13 different SBBFs

%% TODO: mention name ``sectorized Bloom filters'' from Columbia group (Kenneth A. Ross?)

The insert and lookup operations in standard Bloom filters access $\lg (1/\varepsilon)$ bits in an array of size $m$ that stores $m \ln 2 / \lg(1/\varepsilon)$ distinct elements~\cite{bloom-original}.
These cause $\lg (1/\varepsilon)$ cache misses and require the same number of hash function applications.
Block Bloom filters reduce the number of cache misses to 1~\cite{block-bloom}.

Each block Bloom filter is implemented as an array of non-overlapping blocks.
Each block is itself a Bloom filter.
Blocks are no larger than a single cache line in size.
To insert a key, the key is hashed to select the block to use, mapping a key $x$ to $h(x) \bmod m/B$, where $h$ is the hash function, $m$ is the size of the block Bloom filter and $B$ is the size of each block.

Once a block is selected, it is used as a ``split'' Bloom filter~\cite{split-bloom}.
In a standard Bloom filter, to insert a key $x$, $k = m \ln 2 / n$ hash functions are applied to $x$, and each bit $h_k(x) \bmod B$ is set.
In a split Bloom filter, the filter is split into $k$ equal-sized non-overlapping ``lanes'', each of size $L$.
Upon insertion, the bits $i L + (h_i(x) \bmod L)$ for $0 \le i < k$ are set.
See Figure~\ref{sbbf-diagram}.

\begin{figure}
  \includegraphics[width=\columnwidth]{sbbf-diagram}
\caption{\label{sbbf-diagram}
A diagram of a split block Bloom filter with $k = 8$ and $B = 256$.
When inserting an element, a single bit is written in each of eight 32-bit lanes.
Those lanes are stored contiguously in a single block, and each block fits within a single cache line.
The bit to set in each lane is calculated by hashing the key eight times.
}
\end{figure}

When a block Bloom filter is used with block size $B = 256$, $k = 8$ hash functions, and lane size $L = 32$, it is possible to use SIMD instructions to perform eight hash function computations at once, set eight bits at once, or check eight bits at once.
%in what Polychroniou and Ross call ``horizontal vectorization''~\cite{horizontal}.
The resulting Bloom filter has constant-time branch-free insert and lookup and is consistently faster than a cuckoo filter of the same size (See Figures~\ref{lookup-both}~and~\ref{arm-lookup-both})~\cite{cuckoo-filter-github,ultra-fast,overtakes,impala-bloom}.

Taffy block filters use split block Bloom filters as the building block to make an extensible filter with lower query time than a traditional Bloom filter would require in the same application.

\subsection{Cuckoo hashing}

Cuckoo hashing is a method of collision resolution in open-address\-ing hash tables that assigns each key a small set of slots it can occupy~\cite{cuckoo-journal}.
In its simplest form, a cuckoo hash table consists of two arrays of size $(1 + \varepsilon)n$ to store a set of $n$ keys, for $0 < \varepsilon < 1$.
Each key is assigned one possible slot per array via the application of two hash functions on the key.
To look up a key, both slots are checked.

Inserting a key is more complex.
If neither slot for storing a key is empty, one of the two occupying keys is evicted and replaced by the key being inserted.
Now the victim of the eviction is in turn inserted.
With high probability, eventually the evictions find an empty slot and the chain of evictions ends.

This bounds the occupancy of the table below 50\%~\cite{cuckoo-journal}.
To improve the fill factor, cuckoo tables are usually implemented with finite-sized buckets instead of slots, where buckets can hold two or more elements~\cite{buckets}.
Cuckoo filters, taffy cuckoo filters, and minimum taffy cuckoo filters all use buckets of size four~\cite{cuckoo}.
In cuckoo filters, this raises the occupancy limit from 50\% to over 90\%~\cite{load-thresholds}.

\subsection{Succinct dictionaries with quotienting}
\label{quotienting}

Maps of size $n$ with keys from a universe of size $U$ can be na\"ively stored in $n \lg U$ bits by storing every element in an array (with any order) of size $n$.
Space can be saved using a technique called ``quotienting''~\cite{knuth,quotient-filter}.
The basic construction can be illustrated as follows:
first, an array of size $n$ is created in which each array slot can hold an arbitrary number of keys~\cite{raman-practical}.
Then, a key pair $k$ is stored in slot $k \bmod n$.
Additionally, instead of storing $k$ explicitly, $\lfloor k / n \rfloor$ is stored;
$k \bmod n$ is the {\em implicitly-stored} part of they key and $\lfloor k / n \rfloor$ is the {\em explicitly-stored} part of the key.
%% The full value of $k$ can be reconstructed as $(k \bmod n) + n \lfloor k / n \rfloor$.
Because only $\lfloor k / n \rfloor$ is stored as the key, only $\lg U - \lfloor \lg n \rfloor$ bits are required to store it.
Coming back to the array, this reduces the total storage required to $n (\lg U - \lfloor \lg n \rfloor)$.
See Figure~\ref{quotienting-figure}.

\begin{figure}
\includegraphics[width=\columnwidth]{quotienting-diagram}
\caption{\label{quotienting-figure}
Quotienting with $n=4$ and all buckets holding exactly one element.
The column on the left represents a set of values in $\ints_{128}$, with each element taking 7 bits to store.
The column in the middle shows another way of representing the same set as two parts per element: one of the lower order two bits and another of the higher order five.
The column on the right stores the two low-order bits implicitly and the high order five bits explicitly.
}
\end{figure}

This technique is used with linear probing as the collision resolution mechanism in quotient filters~\cite{quotient-filter}.
Quotienting can also be used with cuckooing as the collision resolution mechanism.
Cuckoo hash tables maintain $L \ge 1$ potential locations for each key, each of which could be stored in any of its potential locations~\cite{cuckoo-journal}.
%When a new element is inserted, if its locations are full, it evicts an existing element out of one of the locations; that evicted element must then be placed in one of its $L$ locations, and so on.
Because more than one hash function is used and because eviction occurs, it must be possible to translate from a location-element pair to an alternate location-element pair for the same key
%% Let $k$ be the number of locations and let $h_i$ for $0 \le i < k$ be the hash functions that map

%% It thus

%% The quotienting reconstruction of the key described above does not fully translate to cuckoo hash tables: reconstruction requires a single slot number.
%% If reconstruction followed the pattern above, it would be possible to reconstruct two non-equal keys for the same element, just depending on which of its two or more slots the element was stored in.

Backyard cuckoo hashing handles this by using $L$ different invertible hash functions~\cite{backyard}.
Upon insertion, each new key is first hashed with the $L$ hash functions, then $L$ locations are identified via quotienting.
That is, an element $x$ maps to $h_i(x)$ for $i < L$, and then the slot $h_i(x) \bmod n$ is checked.
When an element is evicted, it is moved from $h_i(x)$ to some other $h_j(x)$, $j \ne i$.
In order to calculate $h_j(x)$ from $h_i(x)$, each hash function $h_i$ is a permutation with an inverse, so $h_j(x)$ can be calculated as $h_j(h_i^{-1}(h_i(x)))$.

Taffy cuckoo filters and minimal taffy cuckoo filters are practical implementations of the theory of quotienting cuckoo hash tables.

%% TODO: make one hash permutation the identity?

\subsection{Filters that can grow}

Pagh \etal{} describe two constructions to support extensible filters~\cite{psw}.
The first is implemented as a series of succinct dictionaries.
Common similar constructions use Bloom filters and exponentially decreasing false positive probabilities in each subsequent filter in order to bound the total false positive rate.
That is, they create a sequence of Bloom filters with the following pairs for the false positive probability and expected number of distinct values:

\[
\langle \varepsilon / 2, 2 \rangle,
 \langle \varepsilon / 4, 4 \rangle,
 \langle \varepsilon / 8, 8 \rangle,
 \langle \varepsilon / 16, 16 \rangle,
 \ldots
\]

This leads to a storage footprint of more than $(\lg n + \lg (1/\epsilon)) / \ln 2$ bits per element and a query time of $O(\lg^2 n + \lg (n/\varepsilon))$.
Pagh \etal{} reduce the lookup cost to $O(\lg n)$ by using a dictionary like Raman and Rao's that has $O(1)$ query time per filter~\cite{psw,succinct}.
They also reduce the space usage to $O(\lg \lg n + \lg (1/\varepsilon))$ bits per element by using the sequence $\langle O(\varepsilon / i^2),  2^i \rangle$, rather than  $\langle \varepsilon / 2^i,  2^i \rangle$.
See Figure~\ref{pagh-1-diagram}.

\begin{figure}
\includegraphics[width=\columnwidth]{pagh-1-diagram}
\caption{\label{pagh-1-diagram}
Pagh \etal{}'s first construction.
In this construction, $\lceil\lg (n-1) \rceil$ dictionaries are maintained with exponentially increasing capacities and logarithmically increasing bit widths.
The false positive probability of the $i$th dictionary, counting from $1$, is $6 \varepsilon / i^2 \pi^2$, and the sum of the false positive probabilities is $\le \varepsilon$.
The lookup operation requires a dictionary lookup in $\lceil\lg(n-1)\rceil$ dictionaries.\\
In this diagram, columns of blocks represent dictionaries.
The caption under a column is the type of the elements in the dictionary. For instance, the block with four rows in its column stores bit strings of length $\lg (3^2 \pi^2 / 6 \varepsilon)$.
Quotienting (see Section~\ref{quotienting}) and other space factors are not visible; neither are lookup or insert patterns.
}
\end{figure}

Pagh \etal{} also present a filter with the same space usage but $O(1)$ query time~\cite{psw}.
This filter maintains a map where the keys are bit strings of length $\lceil \lg n \rceil + \lg (1/\varepsilon) + 2$ and the values are bit strings of length up to $\lg \lg N$, where $N$ will be the largest size of the data structure.
(This definition of $N$ is not a problem in practice, as using $U$, the size of the universe of keys, should be sufficient for integer keys. For non-integer keys, they must be hashed down to an integer in order to use these structures, and using the universe of the set of integers each key is hashed to also works well.)
After every $2^i$ insertions, a new map is created where the keys are one bit longer.
The extra bit in the key is acquired by ``stealing'' a single bit from the value.
See Figure~\ref{pagh-diagram} and Section~\ref{tcf}.

\begin{figure}
\includegraphics[width=\columnwidth]{pagh-diagram}
\caption{\label{pagh-diagram}
Pagh \etal{}'s second construction.
In this construction of a growable filter, when a filter contains $n$ items, it is stored as a dictionary in which the keys are bit stings of length $\lceil \lg n \rceil + \lg (1/\varepsilon) + 2$ and the values are bit strings of length up to $\lg \lg N$, where $N$ is the largest the filter will grow to.
When $\lceil \lg n \rceil$ increases, the high-order bit of the value is appended to the key as a new low-order bit, thus extending the key length.
Pagh \etal{} show that the fpp of such a dictionary is no more than $\varepsilon$ as long as $n < N$.
}
\end{figure}

Taffy filters extend the work of Pagh \etal{} to support Bloom filters in the first construction and space-efficient cuckoo hashing in the second.
This work also implements the Pagh \etal{} structures for the first time and recommends circumstances under which each construction should be used.

\subsection{Compact extensible dictionaries}

Hash tables that are used to accommodate sets without a size known in advance typically do so by doubling in capacity.
This means that at least 50\% of the space goes unused at points, with an average unused percentage of at least 25\%.
Constructions like that of Raman and Rao are able to mitigate this, but they are largely theoretical~\cite{succinct}.
Instead, Maier \etal{} use the cuckoo hashing evict operation to incrementally resize a hash table~\cite{dysect}.
First, the ``DySECT'' table, as they call it, is broken up into equal sized sub-arrays that can be resized independently.
When the table gets close to full, exactly one of the sub-arrays is doubled in size.
This frees up room that's available in future eviction sequences, and the new space will slowly be filled.
Eventually all arrays will have been doubled in size, thereby causing the whole table to have doubled in size without going through a phase with as low as 50\% space usage.

Taffy filters extend quotienting-based dictionaries to DySECT tables for the first time.

\section{Taffy block filters}
\label{pbf}

The first construction from Pagh \etal{} consists of a set of sub-filters of geometrically decreasing false positive probabilities but exponentially increasing size~\cite{psw}.
The $i$th sub-filter is initialized to have false positive probability $6 \varepsilon/(i^2 \pi^2)$ when filled to a capacity of $2^i$.

As Pagh \etal{} describe it, this filter is constructed with only the first sub-filter in place.
Inserts take place on the most recently added sub-filter (which is the largest), while lookups are performed by performing a lookup in each sub-filter until the element is found or there are no more sub-filters to search.
Once $2^i$ inserts have taken place, a new sub-filter is initialized and added to the collection.

Using traditional Bloom filters, the lookup cost would be

\[
\begin{array}{r c l}
\displaystyle\sum_{i=1}^{\lg n} \lg (i^2 \pi^2 /(6 \varepsilon)) & = &
 \displaystyle\sum_{i=1}^{\lg n} \lg (i^2) + \lg( \pi^2) - \lg 6 + \lg (1/\varepsilon) \\
& = & \Theta(\lg^2 n + \lg n \lg (1/\varepsilon))
\end{array}
\]

Instead, Pagh \etal{} use dictionary-based filters that support constant-time lookup -- such as cuckoo filters or Raman and Rao's dictionary -- rather than Bloom filters~\cite{succinct,psw}.
This reduces the lookup time to $O(\lg n)$.

{\bf Taffy block filters} (``TBFs'') use split block Bloom filters to keep the lookup time logarithmic and independent of $\varepsilon$.
See Section~\ref{eval} for performance of TBFs compared to pre-sized split block Bloom filters.

%% TBFs cannot perform the union operation without significant increases in false positive probability.
%% For instance, in two split block Bloom filters of size $m$ where each level has $\delta m$ its bits set, the expected number of bits set in their union is $(2 \delta - \delta^2) m$.
%% Each of the input filters has false positive probability $\delta ^ 8$, while the output filter has false positive probability of nearly $2^8$ times that.

\section{Taffy cuckoo filters}
\label{tcf}

%% Taffy cuckoo filters (``TCFs'') are approximate membership query structures that offer the following operations:

%% \begin{itemize}
%% \item {\bf Create} initializes an empty filter with the given size.
%%   %and false positive probability, and expected {\em growth factor}.
%%   %A growth factor is the maximum number of doublings the structure must be able to undergo while maintaining the given false positive probability.
%% \item {\bf Insert} adds a key to the filter.
%%   This takes $O(1)$ expected time.
%%   It can fail when the filter is nearly full.
%% \item {\bf Lookup} takes a key and returns \verb|True| if the key is in the structure and \verb|False| with probability $1-\varepsilon$ if the key is not in the structure.
%%   It takes $O(1)$ worst-case time.
%% \item {\bf Upsize} doubles the capacity of the filter in $\Theta(n)$ time.
%% %% \item {\bf Union} adds the items from another filter of size $m$ in $O(m)$ time.
%% %% \item {\bf Intersection} Produces a new filter with only the items in both input filters, taking O(min(m,n)) time
%% \end{itemize}

Taffy block filters require $\lg n$ lookup operations on their sub-filters.
Pagh \etal{}'s second construction requires only $O(1)$ time per lookup, and taffy cuckoo filters are a concrete implementation of that.

Taffy cuckoo filters (``TCFs'') use quotienting cuckoo tables (as in backyard cuckoo hashing~\cite{backyard}) to store their data.
The keys are bit-strings of length $\lfloor \lg n \rfloor + F$ and the values are bit-strings of length $V$, for some fixed $F$ (for ``fingerprint'') and $V$ (for ``value'').
By quotienting in an array of size $\Omega(n)$, each key-value pair can be stored in $\lfloor \lg n \rfloor + F + V - \lg n + O(1)$ bits, for a total space usage of $(F+V)n + O(n)$.
For performance and simplicity purposes, we pick $F + V = 16$, but this is not a requirement of the structure.

%\footnote{This matches the original cuckoo filter implementation, which picks element size (in bits) as either 12 or a power of two.}

%% The quotienting table uses permutation hash functions to transform the input key into two different hash values such that either can be used to reconstruct a portion of the original key.
%% The top $\lg n - O(1)$ bits are omitted from the stored hash values, since these are implicitly stored in the location of the key within the table.
%% In this way, data can be moved from one sub-table to the other via key reconstruction, even without storing the key explicitly.

TCFs are based on cuckoo tables and have two sides of slots.
TCFs also use bucketing for cuckoo hashing~\cite{buckets}.
Each side of a TCF comes equipped with a permutation on bit-strings of length $\lg n + F - O(1)$.
A key-value pair $(x, y)$ is stored in one of two buckets: the one in side 0 pointed to by the high-order $\lg n - O(1)$ bits of $P_0(x)$ or the one in side 1 pointed to by the high-order $\lg n - O(1)$ bits of $P_1(x)$, where $P_i$ is the permutation associated with side $i$.

More concretely, an element consists of two groups of bits: one of size 10, and the other of size up to 5.
The former is called the fingerprint and the latter is called the tail.
A bucket consists of four (possibly empty) slots, each of which can hold one element.
A side consists of $2^k$ buckets for some $k$ as well as a permutation on $\ints_2^{k+10}$. %\footnote{$\ints_2^m$ and $\ints_{2^m}$.}
A TCF consists of two sides and one hash function that produces a 64-bit key.
The two sides have the same number of buckets but different permutations.
%See Listing~\ref{tcf-types}.

%% \begin{figure*}
%% \begin{lstlisting}[escapeinside={`}{`},label=tcf-types,
%%     caption={The types of a TCF.
%%       \texttt{T}$_\bot$ means the type \texttt{T} extended with the element $\bot$, indicating ``null'' or ``empty''.
%%       $S_i$ is the symmetric group on $\ints_i$ -- the set of all permutations on $\ints_i$.
%%       %As above, pseudocode will not make a distinction between $\ints_2^i$ and $\ints_{2^i}$.
%%       \texttt{T[n]} denotes an array of $n$ values of type $T$.
%%       $A \dotcup B$ represents a tagged disjoint union of $A$ and $B$; even if $A \subseteq B$, $A \dotcup B \ne B$.
%%       Structs are denoted by curly brackets \{\}, and members of structs can be referenced by their name or by their type.
%%       For instance, if \texttt{s} is a \texttt{Side(k)}, then \texttt{s.Bucket} and \texttt{s.Permutation} are both meaningful expressions.
%%     }
%%     ]
%% Element := {fingerprint: `$\ints_2^{10}$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`}
%% Slot := Element`$_\bot$`
%% Bucket := Slot[4]
%% Side(k) := {Bucket[2`$^k$`], Permutation: `$S_{2^{k+10}}$`}
%% TCF(U, k) := {Side(k)[2], HashFunction: `$U \to \ints_2^{64}$`}
%% \end{lstlisting}
%% \end{figure*}

\paragraph{Lookup}
A lookup begins by hashing a key with the TCF's hash function. %(Listing~\ref{tcf-lookup}, line 12)
Then the lookup operation does the following:

\begin{enumerate}
\item Applies a permutation to the most-significant $k+10$ bits in the key. %(Listing~\ref{tcf-lookup}, line 14)
\item Reserves the next 5 bits of the key; this will be the key's tail.
\item Using the most-significant $k$ bits in the permuted bits, selects a bucket. %(Line 15)
(The remaining 10 bits in the permuted value are the fingerprint.)
\item Checks to see if any slot contains an element with an identical fingerprint. %(Line 3)
If so, checks if the element's tail is a prefix of the key's tail.
If yes, returns \verb|True|.
Otherwise, returns \verb|False|.
\end{enumerate}

%TODO: endianness problems pervade the pseudocode?

%% \begin{figure*}
%% \begin{lstlisting}[escapeinside={`}{`},
%%     label=tcf-lookup,
%%     caption={
%%       Pseudocode for the lookup operation on TCFs.
%%       \texttt{S[a,~b)} denotes the bits in $S$ starting at location $a$ and continuing through (and including) $b-1$.
%%     }]
%% LookupBucket(fingerprint, tail, bucket) {
%%   for (element : bucket) {
%%     if (element.fingerprint == fingerprint && element.tail IsPrefixOf tail) {
%%       return True
%%     }
%%   }
%%   return False
%% }

%% Lookup(input: U, tcf: TCF(U, k)) {
%%   hashed := tcf.HashFunction(input)
%%   for (side : tcf.Side[0], tcf.Side[1]) {
%%     permuted := side.Permutation(hashed[0, k+10))
%%     bucket := side.Bucket[permuted[0, k)]
%%     if LookupBucket(permuted[k, k+10), hashed[k+10, k+15), bucket) {
%%       return True
%%     }
%%   }
%%   return False
%% }
%% \end{lstlisting}
%% \end{figure*}

In total, lookup searches through eight slots.

\paragraph{Insert}
Insert places the key's fingerprint and tail in one of the eight empty slots corresponding to that key, if one is found.
If none is found, insert selects an occupied slot from the bucket to {\em evict}: the element in this slot will be moved to the other side. %(Listing~\ref{tcf-insert}, Line 19)

The evict operation first reconstructs the high order $k + 10$ bits of the key by concatenating the $k$ bits of the bucket index and the $10$ bits of the fingerprint, then applying that side's permutation in reverse to the value. %(Line 23--24)
Using the same tail (this does not get permuted), the evict operation then inserts the evicted data into the opposite side;
this continues until an empty slot is encountered %(Line 18).

%% \begin{lstlisting}[escapeinside={`}{`},
%%     label=tcf-insert,
%%     float,
%%     caption={
%%       Pseudocode for the insert operation on TCFs
%%   }]
%% InsertBucket(fingerprint, tail, bucket) {
%%   new_element := {fingerprint, tail}
%%   for(slot : bucket) {
%%     if (slot == `$\bot$`) {
%%       slot := new_element
%%       return `$\bot$`
%%     }
%%   }
%%   swap(new_element, RandomSlotIn(bucket))
%%   return new_element
%% }

%% InsertSide(tcf: TCF(U,k), side: `$\ints_2$`, hashed: `$\ints_2^{k+10}$`,
%%            tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
%%   permuted := side.Permutation(hashed)
%%   bucket := side.Bucket[permuted[0, k)]
%%   slot = InsertBucket(permuted[k, k+10), tail, bucket)
%%   if (slot == `$\bot$`) return
%%   Evict(tcf, side, permuted[0, k), slot)
%% }

%% Evict(tcf: TCF(U,k), side: `$\ints_2$`, bucket_index: `$\ints_{2^k}$`,
%%       element) {
%%   permuted := Concat(bucket_index, element.fingerprint)
%%   hashed := tcf[side].Permutation`$^\texttt{-1}$`(permuted)
%%   InsertSide(tcf, 1 - side, hashed, element.tail)
%% }

%% InsertTCF(input: U, tcf: TCF(U, k)) {
%%   if (Lookup(input, tcf)) return
%%   hashed := tcf.HashFunction(input)
%%   InsertSide(tcf, 0, hashed[0, k+10), hashed[k+10, k+15))
%% }
%% \end{lstlisting}

\paragraph{Upsize}
When a TCF is nearly full, inserts may fail.
This is identical to the situation with cuckoo filters.
When this happens, the \texttt{Upsize} method must be called to double the size of the structure. %(Listing~\ref{tcf-upsize})

The upsize operation begins by creating a new TCF. %(Line 9)
To transfer the data from the older to the newer TCF, upsize uses a modified version of the evict algorithm.
Upsize first reconstructs the $k+10$ bits of the key that were used to construct the bucket index and fingerprint. %(Line 14)

Then a bit is ``stolen'' from the tail and appended onto the end of the key. %(Line 3)
The high order bit of the tail is removed from the tail and added to the low-order end of the key.
Since the tail was taken unaltered from the key, this gives $k+11$ bits of the original key.
The new tail is now decreased in size by one.
The key and the new tail of it can now be inserted into one of the sides of the new TCF as described above. %(Line 17)

This works as long as the tail has positive length.
If the tail has length zero, there is nothing to steal from.
Instead, two candidate keys are created from the reverse-permuted $k+10$ bits by appending a zero and a one. %(Listing~\ref{tcf-upsize}, line 5)
It's indeterminate which one of these was in the original key, so both are inserted. %(Line 20)
Pagh \etal{} show that the fpp remains less than $2^{-F+2}$~\cite{psw}.

%% \begin{lstlisting}[escapeinside={`}{`},float,label=tcf-upsize,
%%     caption={Pseudocode for the upsize operation on TCFs.
%%   }]
%% Steal(head, tail) {
%%   if (|tail| > 0) {
%%     return Concat(head, tail[0]), tail[1, |tail|)
%%   }
%%   return Concat(head, 0), Concat(head, 1)
%% }

%% Upsize(tcf: TCF(U, k)) {
%%   result := new TCF(U, k + 1)
%%   for (side : tcf) {
%%     for (bucket_index : `$\ints_{2^k}$`) {
%%       for (element : side.Bucket[bucket_index]) {
%%         permuted := Concat(bucket_index, element.fingerprint)
%%         hashed := side.Permutation`$^\texttt{-1}$`(permuted)
%%         if (|tail| > 0) {
%%           (head, tail) := Steal(hashed, element.tail)
%%           InsertSide(result, 0, head, tail)
%%         } else {
%%           for (longhead : Steal(hashed, element.tail)) {
%%             InsertSide(result, 0, longhead, element.tail)
%%           }
%%         }
%%       }
%%     }
%%   }
%%   tcf = result
%% }
%% \end{lstlisting}

\paragraph{Union and Intersection}
In addition to lookup, insert, and upsize, TCFs also support iterating over all elements.
Iteration yields all of the recoverable hashed bits by performing the quotienting in reverse (combining element locations with element fingerprints), then appending the tail. %(See Listing~\ref{tcf-iterate})
Combined with lookup and insert, iterate enables union and intersection operations.

%% TODO: talk about invertable Bloom filters

%% TODO: talk more about union in the database application of build/probe side filters

%% \begin{lstlisting}[escapeinside={`}{`},float,label=tcf-iterate,
%%     caption={Pseudocode for iteration on TCFs; uses coroutines
%%   }]
%% Iterate(tcf: TCF(U, k)) {
%%   result := new TCF(U, k + 1)
%%   for (side : tcf) {
%%     for (bucket_index : `$\mathbb{Z}_{2^k}$`)
%%       for (element : side.Bucket[bucket_index]) {
%%         permuted := Concat(bucket, element.fingerprint)
%%         result := side.Permutation`$^\texttt{-1}$`(permuted)
%%         yield Concat(result, element.tail)
%%       }
%%     }
%%   }
%% }
%% \end{lstlisting}

For union, taffy cuckoo filers iterate over the smaller filter and insert each element into the larger filter, unless they are already present.
For intersection, TCFs iterate over the smaller filter and lookup each element in the larger filter.
If the lookup fails, iteration proceeds to the next element.
Otherwise, the longer bit string of the two elements is inserted into the intersection.
Note that this may sometimes be the element from the smaller TCF, since the tails have variable length

%% TODO: pseudocode and an analysis of the fpp.
%% For union, it should be the sum of the input fpp, minus their product, but need to explain that, since we earlier said all filters have the same low fpp.
%% For intersection, the situation is tricky.
%% We can depend on the inserted strings to be at least as long as the shortest strings in the larger dictionary.

This support for union and intersection is in contrast to TBFs, which do not support iteration or intersection.
TBFs do support union, but at a cost: while union operations on TCFs produce a filter with the sum of the false positive probabilities of the input filters, unions on TBFs perform like unions on Bloom filters.
If one Bloom filter has a fraction $\alpha$ of its bits set and another $\beta$, the union has $\alpha + \beta - \alpha \beta$ of its bits set in expectation.
%This increases the probability of a false positive from $\alpha^k + \beta^k$, where $k$ is the number of hash functions, to $(\alpha + \beta - \alpha \beta)^k$.
For instance, when $\alpha = \beta = 0.5$ and $k = 8$, the false positive rate goes up by a factor of $0.75^8/0.5^8 \approx 25.63$. %, much more than the factor of $2$ with TCFs.

\paragraph{Freeze and Thaw}
TCFs also support {\em freeze} and {\em thaw} operations.
Freeze reduces the space consumption of a TCF from $O(\lg \lg N + \lg (1/\varepsilon))$ to $O(\lg (1 / \varepsilon))$ bits per item, where $N$ is the largest size the structure will grow to.
It does so by recreating the structure as a TCF with tail length capacity $0$.
Thaw simply turns a frozen structure into an unfrozen structure by recreating a TCF with tail length capacity $\lg \lg N$ in which all of the tails are empty.
This allows new inserts to take place while capturing their tails.

%% After thaw, the filter looks dangerously skinny.
%% Since none of the slots have non-empty tails, it appears the false positive probability may rise as more items are added and upsize calls duplicate all keys that were present at the time of thaw.
%% However, the false positive probability actually does remain below $\varepsilon$.

%% Let $k$ be the number of upsize calls after thaw.
%% The set now occupies $n2^k$ slots, where $n$ was the size of the frozen set.
%% Since the thawed data structure starts out of capacity $n$, the number of elements inserted since thawing is at least $n2^{k-1}$, so the number of elements is still linear in the number of times insert has been called since the thaw.
%% Pagh \etal{} show that the number of elements in a structure that has never been thawed and frozen is linear in the number of times insert has been called.
%% This the total capacity is $O(n2^k)$ -- linear in the number of inserts.

%% TCFs support a union operation as well, which operates by iterating over the smaller TCF and inserting into the larger.
%% The iteration is able to partially reconstruct the original hashed key (before permutations) using the same method as in upsize in Figure~\ref{tcf-upsize}, lines 10-14.
%% Although this does not allow the insert method to be used directly, a variation on insert (Figure~\ref{tcf-insert}, line 13-19) can insert a pair of a prefix of a hashed value and a tail.
%% Like upsize, union then steals bits from the tail, except union keeps stealing until the hash value is as long as needed to fit with the insert of the TCF being uniond into.
%% This results in a false positive probability that is the sum of the false positive probabilities of the two input TCFs.

%% TODO:  union benchmarks

%% TODO: union proofs

%% % TODO: intersection

%% \begin{figure}
%% \begin{lstlisting}[escapeinside={`}{`}]
%% InsertUnion(tcf: TCF(U,k), side: `$\ints_2$`, hashed: `$\ints_2^{j+10}$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
%%   if (j == k) return InsertSide(tcf, side, hashed, tail)
%%   if (|tail| > 0) {
%%     (head, tail) := Steal(hashed, tail)
%%     return InsertUnion(tcf, side, head, tail)
%%   }
%%   for (longhead : Steal(hashed, tail))
%%     InsertUnion(tcf, side, longhead, tail)
%%   }
%% }

%% Union(from: TCF(U, k), to: TCF(U, j)) {
%%   if (k > j) return Union(to, from)
%%   for (side : {0, 1}) {
%%     for (bucket_index : `$\ints_{2^k}$`) {
%%       for (element : from.Sides[side].Bucket[bucket_index]) {
%%         permuted := Concat(bucket_index, element.fingerprint)
%%         hashed := from[side].Permutation`$^\texttt{-1}$`(permuted);
%%         InsertUnion(to, side, hashed, element.tail)
%%       }
%%     }
%%   }
%% }
%% \end{lstlisting}
%% \caption{
%% Pseudocode for the union opeation on TCFs.
%% The two input TCFs must have the same hash function.
%% }
%% \end{figure}


% \subsection{Space and false positive analysis}



%% In~\cite{psw}, the dictionary is broken up into subsequences of size $2^i$.
%% Each subsequence $S_i$ contains the inserted keys with insertion order number in $[2^i, 2^{i+1})$.
%% Only when transitioning subsequences is the steal operation performed.
%% However, in PCFs, transitions happen when the dictionary is near full.
%% This does not necessarily correspond to the sequences.
%% Firstly, the $i$th dictionary cannot hold $2^i$ keys, just $\alpha 2^i$, where $\alpha$ si the ``fill factor''.
%% Second, the dictionary is sized according to the number of entries, while the structure in~\cite{psw} is sized according to the number of insertions.
%% These differ, as the number of entries for a sequence grows as the sequence lengthens as a result of the steals against an empty tail.

%% The false positive probability depends on the number of elements with different tail lengths.
%% Specifically, in a dictionary with $n$ elements where $p(i)$ is the number of elements with tail of length $i$, the false positive probability is less than

%% \[
%% 8 n^{-1} \sum_{i \le 7} 2^{-i-8}p(i)
%% \]

%% \[
%% 2^{-5} n^{-1} \sum_{i \le 7} 2^{-i}p(i)
%% \]

%% In any insertion sequence $I$, let $\alpha_I$ be the smallest number such that a dictionary with $\alpha_I x$ entries and capacity for $x$ entries is upsized.



%% \begin{theorem}
%%   $p(i)$
%% \end{theorem}~\begin{proof}
%% Assuming the dictionary starts with both sides having Bucket arrays of size 1, and thus having an element capacity of 8, the first $8 \alpha$ elements are inserted before any upsizing and therefore have tails of the full length, 7.
%% In this first phase, a candidate gets compared to at most $8 \alpha$ elements and has a $2^{-15}$ chance of collision with each, for a false positive probability (``fpp'') of less than $2^{-12}$.
%% Once the table has resized to having capacity 16, at most 8 of the slots are filles with tails of length 6.
%% The addition of at most 8 more items of length 7 makes

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 6\\
%% %% 8 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is less than $8 \cdot 16^{-1} (2^{-14}\cdot 8 + 2^{-15}\cdot 8) = 2^{-12} + 2^{-13}$.
%% %% Repeating, we get

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 5\\
%% %% 8 & i = 6\\
%% %% 16 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is less than $8 \cdot 32^{-1} (2^{-13}\cdot 8 + 2^{-14}\cdot 8 + 2^{-15}\cdot 16) = 2^{-12} + 2^{-13} + 2^{-13}$.

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 4\\
%% %% 8 & i = 5\\
%% %% 16 & i = 6\\
%% %% 32 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is $8 \cdot 64^{-1} (2^{-12}\cdot 8 + 2^{-13}\cdot 8 + 2^{-14}\cdot 16 + 2^{-15}\cdot 32) = 2^{-12} + 2^{-13} + 2^{-13} + 2^{-13}$.

%% %% This pattern repeats until and including

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 0\\
%% %% 2^{2+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% when $n = 2^{10}$ after inserting $2^10$.

%% %% so the fpp is less than $2^{-12} + 6 \cdot 2^{-13}$

%% %% After that, the next $p(i)$ is

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 3 \cdot 2^3 & i = 0\\
%% %% 2^{10} - 8 & i = 7 \\
%% %% 2^{3+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 4 \cdot 2^4 & i = 0\\
%% %% 2^{10} - 2\cdot2^2 & i = 6 \\
%% %% 2^{11} - 3\cdot 2^3 & i = 7 \\
%% %% 2^{4+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 9 \cdot 2^9 & i = 0\\
%% %% 2^{10} - 2 \cdot 2^2 & i = 1 \\
%% %% 2^{11} - 3 \cdot 2^3 & i = 2 \\
%% %% 2^{12} - 4 \cdot 2^4 & i = 3 \\
%% %% 2^{13} - 5 \cdot 2^5 & i = 4 \\
%% %% 2^{14} - 6 \cdot 2^6 & i = 5 \\
%% %% 2^{15} - 7 \cdot 2^7 & i = 6 \\
%% %% 2^{16} - 8 \cdot 2^8 & i = 7 \\
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% a  & i = 0\\
%% %% b & i = 1 \\
%% %% c & i = 2 \\
%% %% d & i = 3 \\
%% %% e & i = 4 \\
%% %% f & i = 5 \\
%% %% g & i = 6 \\
%% %% h & i = 7
%% %% \end{cases}
%% %% \]

%% $p$ with total size $m = 2^j$

%% changes to

%% \[
%% p(i) =
%% \begin{cases}
%% p(1) + 2p(0) & i = 0\\
%% p(2) & i = 1 \\
%% p(3) & i = 2 \\
%% p(4) & i = 3 \\
%% p(5) & i = 4 \\
%% p(6) & i = 5 \\
%% p(7) & i = 6 \\
%% 2^j - p(0) & i = 7 \\
%% \end{cases}
%% \]

%% with total size $2^{j+1}$.

%% The fpp changes from

%% \[
%% \varepsilon 8 \cdot 2^{-j} \left(\sum p(i) 2^{-i}\right)
%% \]

%% to

%% \[
%% 2^{-j + 2} \left(p(1) + 2p(0) + (2^j - p(0))2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% if $p(0) < 3 \cdot 2^{j-2}$, then this is at most

%% \[
%% 2^{-j + 2} \left(p(1) + 2 \cdot  3 \cdot 2^{j-2} + (2^j -  3 \cdot 2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% 2^{-j + 2} \left(p(1) + 3 \cdot 2^{j-1} + (2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% 2^{-j+2} \left(p(1) + 3 \cdot 2^{j-1} + (2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% \varepsilon (2^{-j+2}p(1) + 3 \cdot 2^{1} + 2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i-j+3}
%% \]

%% \[
%% \varepsilon (2^{-j+2}p(1) + 12 + 2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i-j+3}
%% \]

%% Everything except 12 is vanishing, so 

%% \[
%% \begin{cases}
%% 2^{10} - 2 \cdot 2^2 & i = 1 \\
%% 2^{11} - 3 \cdot 2^3 & i = 2 \\
%% 2^{12} - 4 \cdot 2^4 & i = 3 \\
%% 2^{13} - 5 \cdot 2^5 & i = 4 \\
%% 2^{14} - 6 \cdot 2^6 & i = 5 \\
%% 2^{15} - 7 \cdot 2^7 & i = 6 \\
%% 2^{16} - 8 \cdot 2^8 & i = 7 \\
%% \end{cases}
%% \]


%% \[
%% p(i) =
%% \begin{cases}
%%  -2 \cdot 2^2 + 10 \cdot 2^{10} & i = 0\\
%% 2^{11} - 3 \cdot 2^3 & i = 1 \\
%% 2^{12} - 4 \cdot 2^4 & i = 2 \\
%% 2^{13} - 5 \cdot 2^5 & i = 3 \\
%% 2^{14} - 6 \cdot 2^6 & i = 4 \\
%% 2^{15} - 7 \cdot 2^7 & i = 5 \\
%% 2^{16} - 8 \cdot 2^8 & i = 6 \\
%% 2^{17} - 9 \cdot 2^9 & i = 7 \\
%% \end{cases}
%% \]

%% \[
%% p(i) =
%% \begin{cases}
%% - 5 \cdot 2^3 + 11 \cdot 2^{11} & i = 0\\
%% 2^{12} - 4 \cdot 2^4 & i = 1 \\
%% 2^{13} - 5 \cdot 2^5 & i = 2 \\
%% 2^{14} - 6 \cdot 2^6 & i = 3 \\
%% 2^{15} - 7 \cdot 2^7 & i = 4 \\
%% 2^{16} - 8 \cdot 2^8 & i = 5 \\
%% 2^{17} - 9 \cdot 2^9 & i = 6 \\
%% 2^{17} - 9 \cdot 2^9 & i = 6 \\
%% \end{cases}
%% \]

%% with $n = 2^{11} - 2^4 + 3 \cdot ^3$ after inserting $2^11$

%% and then

%% \[
%% p(i) =
%% \begin{cases}
%% 4 \cdot 2^4 & i = 0\\
%% 2^{4+i} & \text{otherwise}
%% \end{cases}
%% \]

%% with $n = 2^{12} - 2^5 + 4 \cdot 2^4$


%% following the pattern

%% \[
%% p(i) =
%% \begin{cases}
%% j \cdot 2^j & i = 0\\
%% 2^{j+i} & \text{otherwise}
%% \end{cases}
%% \]

%% when $n = j \cdot 2^j - 2^{j+1} + 2^{8+j}$ after inserting

%% In this case, the false positive probability is 

%% \[
%% 2^{-5} \cdot (j \cdot 2^j - 2^{j+1} + 2^{8+j})^{-1}(j \cdot 2^j + 2^{-1}2^{j+1} + 2^{-2}2^{j+2} \dots)
%% \]

%% \[
%% 2^{-5} \cdot (j \cdot 2^j - 2^{j+1} + 2^{8+j})^{-1}(j \cdot 2^j + 7\cdot2^j)
%% \]

%% \[
%% 2^{-5} \cdot (2^j(j - 2 + 2^8))^{-1}2^j(j + 7)
%% \]

%% \[
%% 2^{-5} (j - 2 + 2^8)^{-1}(j + 7)
%% \]

%% \[
%% \frac{j+7}{2^5 (j - 2 + 2^8)}
%% \]

%% \end{proof}

%% That is only approximated in PCFs, as the 

%% The calculations of how many slots the accumulation of these doublings produces is the key calculation in the space consumption analysis in~\cite{psw}.
%% That is crucial as well for the false positive rate
%% For the concrete setting here, if the dictionary starts with $k = 0$ (each side having size 1 bucket), then after $m$ upsizes, if $m < 7$, $2^{m+2}(1 - \delta)$ keys have been inserted and the same number are in the dictionary.
%% Half of the tails have length seven, a quarter have length six, and so on.
%% The false positive probability is $2^{m+2}(1 - \delta) \cdot 2^{-m-8}(m \cdot 2^{-8}) = m(1-\delta)2^{-14}$, which is between $6.1E-5$ and $4.2E-4$.
%% TODO: experimentally different. Make graph.
%% %% check to see if the tail in that slot is a prefix of the 

%% %% To look up a key

%% %% Plastic cuckoo filters are based on this filter, but additionally add a value to each key.
%% %% The value is the next $\lg d$ bits of the key, where $d$ is the growth factor.



%% %% Plastic cuckoo filters are based on a type of bloomier filter.
%% %% A bloomier filter is an AMQ supporting lookups, not just membership queries.
%% %% Given a domain and range $D$ and $R$ where $D \subset U$ and $M$ is a map from $D$ to $R$, a query for $x$ in a bloomier filter for $M$ returns $M(x)$ if $x \in D$ and $\bot$ with probability $1-\varepsilon$ when $x \in U \backslash D$, and otherwise returns an arbitrary value in $R$.

%% %% Next we will form a quotienting cuckoo map.
%% %% A quotienting cuckoo map has a domain of $\ints_d$ and a range of $\ints_r$.
%% %% A standard cuckoo map would use $d + r$ bits per unit of capacity.
%% %% A quotienting cuckoo table uses $d + r + \lg b -  \lg m $ where $m$ is the capacity of the table and $b$ is the size of the bucket.


%% %% In an elastic filter, the permuted key has length $\lg \nicefrac{1}{\varepsilon} + \lg m - \lg b$ while the range consists of all sequences of bits of length less than $\lg\lg \nicefrac{U}{n} + O(1)$.
%% %% When the table is nearly full, meaning that further additions would likely fail, the size is doubled.
%% %% This increases the size of the unstored bits by 1, so the number of stored bits would be smaller, as well.
%% %% Instead of letting that happen, a bit is ``stolen'' from the range of sequences of bits.

%% %% This shortens the value in the range, but the range stays the same.
%% %% If there are no bits to steal becuase the value is the empty string, {\em two} new values are generated to insert into the new, larger table: one with a one appended to the stored bits, one with a zero.
%% %% This follows directly from~\cite{psw}.

% TODO: \paragraph{Satelite Data}


\section{Minimal taffy cuckoo filters}
\label{mtcf}

Taffy cuckoo filters suffer from a step-function space usage:
at each point, the structure has a size which is a power of two, sometimes allocating much more space than will be needed. (See Figure~\ref{space-steps}.)
To address this, this section describes a similar structure using DySECT to reduce the space usage to only what is needed~\cite{dysect}.

DySECT is a variant of cuckoo hashing.
A DySECT table consists of some number of subtables, and as the table gets more and more full, it grows by doubling the size of one of its subtables.
Just as in cuckoo hashing, upon an insertion, an element may be evicted.
As new elements are inserted into the table, they evict older elements, and this movement causes the newly-doubled subtable to fill up.

This section proposes minimal taffy cuckoo filters (``MTCFs''), an application of the DySECT idea to quotienting and taffy filters.
Some complications arise:

\begin{enumerate}
  \item Because subtables have different sizes, the bits that are implicitly stored using quotienting varies depends on which part of the table an element is in.
    To address this, fingerprints in MTCFs have variable size.
  \item Because fingerprints have variable size, there must be multiple permutations per side, one for each size of fingerprint.
  \item Because there are multiple permutations per side, a key may be in multiple distinct buckets per side, which decreases the performance and increases the false positive probability.
\end{enumerate}

In an MTCF, each element has a fingerprint of size 8 or 9 and a tail of size up to 5.\footnote{As in TCFs, these constants are picked for performance and ease of implementation.
Any other constants are possible.
}
A bucket consists of four (possibly empty) slots, each of which can hold one element.
A level consists of two arrays of the same size, each with $2^k$ buckets for some $k$.
The table consists of four permutations, one hash function, $32$ levels, and one cursor pointing to some index in the set of levels.
The maximum and minimum $k$ across all levels differ by at most 1.
Levels at location less than the cursor have the larger size.
If all levels have the same size, the cursor must be 0.
See %Listing~\ref{mtcf-types} and
Figure~\ref{mtcf-diagram}.

The permutations are grouped by side, two for each.
The permutations are on values with length $k + 13$ and $k + 14$, where $2^k$ is the size of the smallest table, measured in buckets.

%% \begin{lstlisting}[escapeinside={`}{`},float,label=mtcf-types,
%%     caption={
%%       The types of an MTCF.
%%       A Permutation(k) is a tagged union of permutations; wlog it can be applied to bit strings of known length.
%%   }]
%% MElement := {fingerprint: `$\ints_2^8 \dotcup \ints_2^9$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`}
%% MSlot := MElement`$_\bot$`
%% MBucket := MSlot[4]
%% Level(k) := MBucket[2][2`$^k$`] `$\dotcup$` MBucket[2][2`$^{k+1}$`]
%% Permutation(k) := `$S_{2^{k+13}} \dotcup S_{2^{k+14}}$`
%% MTCF(U, k) := {cursor: `$\ints_{32}$`,
%%                Level(k)[32],
%%                Permutation(k)[2],
%%                HashFunction: `$U \to \ints_2^{64}$`}

%% \end{lstlisting}

\begin{figure}
  \includegraphics[width=\columnwidth]{mpcf-diagram}
\caption{\label{mtcf-diagram}
A diagram of an MTCF.
In this example, $k = 2$, so the larger levels have $2^{k+1} = 8$ buckets on each side, while the smaller levels have four.
In the first bucket of side 0 of the 32nd level, the last three slots are empty.
If the entry were from one of the larger levels above the cursor, then the fingerprint would have to be from $\ints_2^8$, rather than from $\ints_2^8 \dotcup \ints_2^9$
}

\end{figure}

If there are larger and smaller levels, then every element in the larger levels has a fingerprint of size 8, not 9.
This is because the implicitly-stored part of the key is one-bit longer in the larger levels, so the explicitly stored part can be shorter.

In an MTCF, upsize only increases the size of one of the levels, not the whole structure.
As a result, the capacity of the filter tracks more closely the number of entries in the table. (See Figure~\ref{bits-per-item}.)

\paragraph{Lookup}
A lookup operation in an MTCF first applies each of the four permutations to the hashed key.
\begin{itemize}
\item For the permutations on $k + 14$ bits %(Listing~\ref{mtcf-lookup}, line 11),
the first five bits indicate the level %(line 12)
, the next $k$ or $k+1$ indicate the bucket %(line 14)
, and the remaining 8 or 9 bits are the fingerprint.
Lookup proceeds as it does in the TCF case, by checking if fingerprints match and if the stored tail is a prefix of the tail of the key being looked up. %(Lines~15--16.)
\item For the permutations on $k + 13$ bits, the first five bits again indicate the level. %(Line~3.)
\begin{itemize}
\item If the level has tables with $2^{k+1}$ buckets, the permuted key is not used for lookup; to do otherwise would leave only $k+13 - 5 - (k+1) = 7$ bits for the fingerprint, which is not possible.
That key is simply skipped and the lookup continues with the next key. %(Line~4)
\item Otherwise, the level has tables with $2^k$ buckets, and we can proceed as in the $k+14$ case. %(Lines~5--10)
\end{itemize}
\end{itemize}

%% \begin{lstlisting}[escapeinside={`}{`},float,label=mtcf-lookup,
%%     caption={
%%       The lookup operation in an MTCF. Notice that up to four buckets may be searched: between one and two per side.
%%   }]
%% LookupSide(mtcf: MTCF(U, k), side: `$\ints_2$`, hashed: `$\ints_2^{64}$`) {
%%   permuted := mtcf.Permutation[side](hashed[0, k+13))
%%   level_number := permuted[0, 5)
%%   if (level_number `$\ge$` mtcf.cursor) {
%%     bucket := mtcf.Level[level_number][permuted[5, k + 5)]
%%     if (LookupBucket(permuted[k+5, k+13),
%%                      hashed[k+13, k+18), bucket)) {
%%       return True
%%     }
%%   }
%%   permuted = mtcf.Permutation[side](hashed[0, k+14))
%%   level_number = permuted[0, 5)
%%   middle := (level_number `$\ge$` mtcf.cursor) then k+5 else k+6
%%   bucket := mtcf.Level[level_number][permuted[5, middle)]
%%   return LookupBucket(permuted[middle, k+14),
%%                       hashed[k+14, k+19), bucket)
%% }

%% MLookup(mtcf: TCF(U, k), key: U) {
%%   hashed := mtcf.HashFunction(key)
%%   for (side : {0, 1}) {
%%     if (LookupSide(mtcf, side, hashed)) return True
%%   }
%%   return False
%% }
%% \end{lstlisting}

%% Upsize only increases the size of one level.
%% After 31 upsizes, the next one makes all of the levels the same size.
%% At that point, two new permutations must be initialized to handle new longer key prefixes.

\paragraph{Insert}
In insert operations, as in lookup, the first five bits of the permuted item indicate the level. %(Listing~\ref{mtcf-insert}, lines~17, 19, 26, and 47--48)
Just as in TCFs, the insert operation on a bucket produces an eviction. %(Listing~\ref{mtcf-insert}, lines~5--8)
During an evict operation in an insert, an element may move between levels with differently-sized arrays of buckets.
When the fingerprint has size 8 and the level moved {\em from} has a bucket array of size $2^k$ and the level moved {\em to} has a bucket array of size $2^{k+1}$, the number of explicitly stored bits (the fingerprint bits) is now $(k + 8) - (k+1) = 7$.
Since every fingerprint must be of length 8 or 9, a bit must be stolen from the tail. %(Lines~29--36)
As in TCFs, if there are no bits to steal, two new key prefixes are created and inserted, as one of them must be the prefix of the original key. %(Line 34)
See Figure~\ref{mtcf-state-transition}.

\begin{figure}
  \includegraphics[width=\columnwidth]{mtcf-state-transition}
\caption{\label{mtcf-state-transition}
This diagram illustrates the transitions an element in an MTCF can go through when evicted.
% The diagram can be read as ``an element in state A can become an element in state B when evicted''.
The states indicate the lengths of the level, fingerprint, and tail.
For instance, when a level's index is less than the cursor (the center state in this diagram), the length of that level is twice what it would be if its index were higher.
In any of the four states, an eviction may move an element from that state to itself.
For instance, an element in a long level can be evicted to stay in a long level, and the same can be said of an element with a long fingerprint (which must necessarily be in a short level).
For elements in a short level with a short fingerprint and no tail, when they are evicted to an element in a long level, two elements are created, as it is impossible to steal a bit from the empty tail.
}
\end{figure}

%% \begin{lstlisting}[escapeinside={`}{`},caption={Pseudocode for the insert operation on MTCFs},float,label=mtcf-insert]
%% MInsertSide(mtcf: MTCF(U, k), side: `$\ints_{2}$`,
%%             level_number: `$\ints_{32}$`,
%%             bucket_number: `$\ints_{2^k} \dotcup \ints_{2^{k+1}}$`,
%%             fingerprint `$\ints_2^8 \dotcup \ints_2^9$`,
%%             tail: `$\sum_{i \le 5}\ints_2^i$`,) {
%%   bucket := mtcf.Level[level_number][side][bucket_number]
%%   new_slot := InsertBucket(fingerprint, tail, bucket)
%%   if (new_slot == `$\bot$`) return
%%   fingerprint = new_slot.fingerprint
%%   tail = new_slot.tail
%%   permuted := Concat(level_number, bucket_number,
%%                      fingerprint)
%%   hashed := mtcf.Permutation[side]`$^\texttt{-1}$`(permuted)
%%   permuted = mtcf.Permutation[1 - side](hashed)
%%   MInsertPermuted(mtcf, side, permuted, tail)
%% }

%% MInsertPermuted(mtcf: MTCF(U, k), side: `$\ints_2$`,
%%                 permuted: `$\ints_2^{k+13} \dotcup \ints_2^{k+14}$`, tail: `$\sum_{i \le 5}\ints_2^i$`) {
%%   level_number := permuted[0, 5)
%%   if (|permuted| == k + 14) {
%%     middle := if (level_number `$\ge$` mtcf.cursor)
%%               then k+5 else k+6
%%     MInsertSide(mtcf, 1 - side, level_number,
%%                 permuted[5, middle),
%%                 permuted[middle, k+14), tail)
%%     return
%%   }
%%   if (level_number `$\ge$` mtcf.cursor) {
%%     permuteds := {permuted}
%%     middle := k + 5
%%   } else if (|tail| > 0) {
%%     (permuted, tail) := Steal(permuted, tail)
%%     permuteds := {permuted}
%%     middle := k + 6
%%   } else {
%%     permuteds := Steal(permuted, tail)
%%     middle := k + 6
%%   }
%%   for (p : permuteds) {
%%     MInsertSide(mtcf, 1 - side, level_number,
%%                 p[5, middle),
%%                 p[middle, middle + 8), tail)
%%   }
%% }

%% InsertMTCF(mtcf: MTCF(U, k), key: U) {
%%   if (MLookup(mtcf, key)) return
%%   hashed := mtcf.HashFunction(key)
%%   permuted := mtcf.Permutation[0](hashed[0, k+14))
%%   level_number := permuted[0, 5)
%%   middle := if (level_number `$\ge$` mtcf.cursor)
%%             then k+5 else k+6
%%   bucket_number := permuted[5, middle)
%%   fingerprint := permuted[middle, k+14)
%%   tail := hashed[k+14, k+19)
%%   MInsertSide(mtcf, 0, level_number,  bucket_number,
%%               fingerprint, tail)
%% }
%% \end{lstlisting}

The analysis of iterate, union, intersection, and freeze/thaw is identical to that of TCFs.

% Pseudocode for insert and upsize are available in Appendix~\ref{mtcf-appendix}.

%% \section{Applications}
%% \label{applications}
%% \subsection{distributed joins}
%% \subsection{LSM trees}

\section{Evaluation}
\label{eval}

% TODO: evaluate starting taffy filters with more starting elements: maybe 1024

TBFs, TCFs, and MTCFs have been implemented and tested for correctness; this section describes their space usage, false positive probabilities, and performance under the insert and lookup operations.

In each chart, TBFs are configured for a maximum fpp of 0.4\%.
All taffy filters are configured with an initial capacity of 1. %, however the MTCF has a minimum size of 2 bytes/entry $\cdot$ 32 levels $\cdot$ 2 sides/level $\cdot$ 1 entry / side = 128 bytes.
All experiments were performed on both an Intel i7-7800X with 96GB of memory and SMT turned on and an AWS EC2 instance of class m6g.medium with 4GB of memory and a single Graviton2 ARM-based core.
The experiments use Ubuntu 18.04 and 20.04, respectively, and g++ 10 and 9, respectively.

For performance testing, we equip both TCFs and MTCFs with stashes~\cite{stash}.
We set both filters to upsize when they are 90\% full or their stashes have size greater than 4.

For comparison, the graphs also include a cuckoo filter (labeled ``CF'') with fingerprints of size 12 and a split block Bloom filter (labeled ``SBBF'') sized to hold 100 million elements with an fpp of 0.4\%.

\subsection{Space}

A filter with a false positive probability of $\varepsilon$ must take up at least $\lg (1/\varepsilon)$ bits per element~\cite{lower-bound}.
Practical filters use more space.
For instance, Bloom filters use $\lg (1/\varepsilon)/\ln 2$ bits per element, which is about $1.44 \lg (1/\varepsilon)$.
Cuckoo filters and quotient filters use $(\lg (1/\varepsilon) + d) / \alpha$ where $d$ is between 2 and 3, and $\alpha$ is the fill factor, between 80\% - 99\%~\cite{cuckoo,quotient-filter,vector-quotient}.
Static filters that do not support insert -- such as the ribbon filter -- can use nearly optimal space~\cite{ribbon}.

However, Pagh \etal{} showed that filters that can grow, like taffy filters can, must use at least $\lg (1/\varepsilon) + \Omega(\lg \lg n)$ bits per element~\cite{psw}.
Figures~\ref{bits-per-item}~and~\ref{ideal-bits-per-item} show the actual bits per element and $\varepsilon$.

Cuckoo filters cannot grow (without changing the number of bits per slot and doubling the false positive rate).
As such, cuckoo filters are only visible over 10 million elements in Figure~\ref{bits-per-item}, since their bits per key starts out at over 1.2 billion.
The same is true of split block Bloom filters.

Figures~\ref{bits-per-item}~and~\ref{space-steps} show the periodic nature of the space used in taffy filters.
When a filter increases in size, the number of bits per element increases as well.
MTCFs moderate this pattern, but do not eliminate it.



% TODO: graph different MTCF variants based on level size, fingerprint size

% TODO: graph fill factor

% TODO: freeze/thaw operations with ribbon filters

% TODO: compare standard bloom filters, filters with exponentially decreasing fpp


\begin{figure}

  \includegraphics[width=\columnwidth]{bits-per-item}
  \caption{  \label{bits-per-item}
Number of bits (per element) each filter type needs.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{space}
  \caption{
    \label{space-steps}
    The amount of space used by each filter at the given number of keys inserted.
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{ideal-bits-per-item}
  \caption{  \label{ideal-bits-per-item}
    This figure shows $\varepsilon$, the false positive probability.
    %% TODO: explain the sudden jump in taffy filters around 500-5000.
    %% Explain the low fpp of CF and lower of SBBF.
    %% Explain the difference between MTCF and TCF/TBF.
  }
\end{figure}


%% \begin{figure}
%%   \includegraphics[width=\textwidth]{deficiency}
%%   \caption{
%%     There is a lower bound on the amount of space that must be used to form an approximate membership query structure with a given false positive rate: $\lg \nicefrac{1}{\varepsilon}$ bits per element.
%%     Any additional space is, in a sense, ``wasted''.
%%     In another sense, this overstates the waste, as the best dynamic structures, like Bloom filters and semi-sorted cuckoo filters, do not achieve the lower bound.
%%     Lower is better.
%%   }
%% 
%% \end{figure}

%% \begin{figure}
%%   \includegraphics[width=\textwidth]{insert-deficiency}
%%   \caption{
%%     Comparing both the wasted space of each filter type with the absent lookup performance.
%%     Lower left is better.
%%   }
%% \end{figure}


\subsection{Time}

Figures~\ref{insert-time},~\ref{arm-insert-time},~\ref{lookup-both},~and~\ref{arm-lookup-both} show the performance of taffy filter operations.
For inserts, TBFs are the fastest of the three taffy filter variants; they are even faster than the fastest non-taffy variant, split block Bloom filters.
TBFs inserts are faster than the other cuckoo filters because they are simple, branch-free, and induce a single cache miss; they are faster than the pre-sized split block Bloom filter because, while being built, the entirety of the TBF fits in cache until about 10 million elements have been inserted.
This holds true across both machines, x86 and ARM.

For lookups, the situation is more complex.
Of the resizable filters, the taffy cuckoo filter is the fastest once the size of the filter is large enough, while a TBF is otherwise faster.
The MTCF lags behind both.
%One reason for this is that the ARM split block Bloom filters that the TBF is based on only have 128-bit vector units, not 256-bit units like the x86 machine does.

\begin{figure}
  \includegraphics[width=\columnwidth]{insert-cumulative}
  \caption{
    \label{insert-time}
    Insert times for filters, i7-7800X.
    %% TODO: the dip is an artifact; redo with more samples and longer time between timer start \& stop.
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{arm-insert-cumulative}
  \caption{
    \label{arm-insert-time}
    Insert times for filters on an AWS EC2 m6g.medium with an ARM-based Graviton2.
    %% TODO: the dip is an artifact; redo with more samples and longer time between timer start \& stop.
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{lookup-both}
  \caption{
    \label{lookup-both}
    Lookup times for filters, i7-7800X.
    This is for keys that are not found; the corresponding figure for found keys looks nearly identical.
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{arm-lookup-both}
  \caption{
    \label{arm-lookup-both}
    Lookup times for filters on an AWS EC2 m6g.medium with an ARM-based Graviton2.
    This is for keys that are not found; the corresponding figure for found keys looks nearly identical.
  }
\end{figure}

%% \begin{figure}
%%   \includegraphics
%% [width=\columnwidth]
%% {lookup-absent}
%%   \caption{
%%     \label{lookup-absent}
%%     Lookup times for filters when the key is absent.
%%   }
%% \end{figure}

\subsection{Discussion}

The MTCF offers lower space than the other two taffy filters, but its speed is substantially worse.
It has significant insertion time increases when it is hard to find an eviction sequence; in this case consecutive insert operations may call upsize, causing a spike in the graph. (See Figures~\ref{insert-time}~and~\ref{arm-insert-time}.)
This cyclic behavior was noted by Maier \etal~\cite{dysect}.
%This behavior is also seen in the TCF, though less severely.
% Performance for lookup is also cyclic as the fill factor and size of the stashes grow and shrink.

During lookup operations on MTCFs, when the cursor is close to 32, the performance improves as the four potential locations to look for a key are more frequently reduced to two, since the shorter permuted keys are no longer long enough for most of the levels in the structure.
See Figures~\ref{lookup-both}~and~\ref{arm-lookup-both}.

Split block Bloom filters and cuckoo filters are still attractive choices when the size of the set to be approximated is known in advance.
When a growable filter is needed, the application matters quite a bit.
If saving every byte matters, MTCFs are called for.
If union, intersection, or freeze operations are needed, a TCF or MTCF is called for.
Otherwise, a practitioner must ask themselves:

\begin{itemize}
\item Is the workload write-heavy or read-heavy?
  Write-heavy workloads favor TBFs over TCFs.
\item Is the set likely to exceed one million elements (x86) or 10,000 elements (ARM)?
  If yes, a TCF is an attractive choice.
%% \item Are union, intersection, or freeze operations needed?
%%   A TCF or MTCF is the only choice.
%% \item Is the hardware known in advance?
%%   If x86, the TBF may be the best choice.
\end{itemize}

The code for taffy filters is available on GitHub under a permissive open-source license.\ifanon\else\footnote{\url{https://github.com/jbapple/libfilter}}\fi

% TODO: show different level counts and how that decreases the spikiness?

% TODO: what is the fpp of the dictionary, analytically

% TODO: explain permutation swapping

% TODO: explain combining tails

% TODO: show that longer fingerprints lead to higher fill factors

% TODO: Show how much time is spent in Upsize

% TODO: show how stash limits lead to higher insert times and lower lookup times

% TODO: show that the size of the dictionary stays limited, as in~\cite{psw}

\section{Future work}
\label{conclusion}

Future work includes:

\begin{itemize}
\item The stash limitations for the TCF and MTCF are very low - tuned for a linear search.
It is not necessary that the stashes be unstructured - giving structure to the stash, as in backyard cuckoo hashing, could allow the stash to grow and the fill factor to go higher without needing to upsize~\cite{backyard}.

\item There is an additional structure supporting sets of unknown sizes described in Liu \etal~\cite{unknown-prefix}.
This structure is mostly theoretical and depends on a certain type of prefix search structure with large embedded constants.
However, a practical version of this may be possible to build.

\item For the TCF, alternate quotienting dictionaries, including quotient filters, broom filters, or bucketing hash tables (as presented by K\"oppl \etal{}) could potentially improve performance~\cite{raman-practical,broom,quotient-filter}.

\item Testing taffy filters in various applications requiring the use of extensible approximate membership query structures would be of interest.
\end{itemize}

\ifanon
\else
\section*{Acknowledgments}
Thanks to Pedro Vasallo and Alex Breslow for helpful discussions and feedback.
\fi

\bibliographystyle{plain}
% \bibliographystyle{ACM-Reference-Format}
\bibliography{taffy}

%% \pagebreak
%% \appendix
%% \section{MTCF pseudocode}
%% \label{mtcf-appendix}
 
%% %% 1
%% %\begin{figure}%[!htbp]
%% %\caption{The lookup operation in an MTCF.}

%% %\end{figure}
%% %% 2
%% %% \begin{figure}

%% %% \caption{Pseudocode 2 for the insert operation on MTCFs}
%% %% \end{figure}

%% %% 5

%% %% \begin{figure}
%% \begin{lstlisting}[escapeinside={`}{`},caption={Pseudocode for the upsize operation on MTCFs}]
%% UpsizeMTCF(mtcf: MTCF(U, k)) {
%%   old := mtcf.Level[mtcf.cursor]
%%   mtcf.Level[mtcf.cursor] = new Level(2`$^{{k+1}}$`)
%%   mtcf.cursor = mtcf.cursor + 1
%%   for (side : {0, 1}) {
%%     for (bucket_index : `$\ints_{2^k}$`) {
%%       for (element : old.MBucket[side][bucket_index]) {
%%         cursor = cursor - 1
%%         permuted := Concat(cursor, bucket_index,
%%                            slot.fingerprint)
%%         hashed := mtcf.Permutation[side]`$^{\texttt{-1}}$`(permuted)
%%         cursor = cursor + 1
%%         permuted = mtcf.Permutation[side](hashed)
%%         MInsertPermuted(permuted, element.tail, side, mtcf)
%%       }
%%     }
%%   }
%% }
%% \end{lstlisting}

%% \caption{Pseudocode for the upsize operation on MTCFs}
%% \end{figure}

%% \begin{figure}
%% \begin{lstlisting}[escapeinside={`}{`}]
%% MInsertUnion(mtcf: MTCF(U, k), side: `$\ints_2$`, hashed: `$\ints_2^{j+13}\dotcup\ints_2^{j+14}$`,
%%              tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
%%   if (j == k) {
%%     permuted = mtcf.Permutation[side](hashed)
%%     return MInsertPermuted(mtcf, side, permuted, tail)
%%   }
%%   if (|tail| > 0) {
%%     (head, tail) := Steal(hashed, tail)
%%     return MInsertUnion(mtcf, side, head, tail)
%%   }
%%   for (h : Steal(hashed, tail)) {
%%     MInsertUnion(mtcf, side, h, tail)
%%   }
%% }

%% Union(from: MTCF(U, k), to: MTCF(U, j)) {
%%   if (k > j) return Union(to, from)
%%   for (level : `$\ints_32$`)
%%     for (side : {0, 1}) {
%%       for (bucket_index : `$\ints_{2^k}$`) {
%%         for (element : from.Sides[side].Bucket[bucket_index]) {
%%           permuted := Concat(level, bucket_index, element.fingerprint)
%%           hashed := from[side].Permutation`$^\texttt{-1}$`(permuted);
%%           MInsertUnion(to, side, hashed, element.tail)
%%         }
%%       }
%%     }
%%   }
%% }

%% \end{lstlisting}
%% \caption{Pseudocode for the union operation on MTCFs.
%% The input MTCFs must have the same hash function.}
%% \end{figure}

\end{document}

%%  LocalWords:  growable lookups quotienting AMQ Upsize TCF TCF's
%%  LocalWords:  HashFunction LookupBucket IsPrefixOf tcf InsertTCF
%%  LocalWords:  InsertSide InsertBucket RandomSlotIn otherSide TCFs
%%  LocalWords:  Concat upsize doublings DySECT MTCF MSlot MBucket
%%  LocalWords:  upsizes TODO fpp LookupSide mtcf MLookup MInsertSide
%%  LocalWords:  MInsertPermuted permuteds MTCFs InsertMTCF MElement
%%  LocalWords:  UpsizeMTCF TBFs Structs structs SMT Pagh Segev FOCS
%%  LocalWords:  Wieder longhead TBF onn Raman Rao's Rao Maier Gatos
%%  LocalWords:  Vasallo Breslow PVLDB VLDB ISSN logarithmically AWS
%%  LocalWords:  Polychroniou vectorization subtables subtable SBBF
%%  LocalWords:  Graviton resizable ApproxJoin LSM RocksDB
