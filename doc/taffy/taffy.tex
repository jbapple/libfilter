% VLDB journal - https://www.springer.com/journal/778/submission-guidelines Fee for open access $2780 https://www.springer.com/journal/778/open-access-publishing#Fees%20and%20Funding
% JEA - journal https://dl.acm.org/journal/jea/instructions-for-authors Fee for open access $1700 or $1300 https://www.acm.org/publications/openaccess#h-open-access-pricing
% TKDE - Journal https://www.computer.org/csdl/journal/tk can't figure out if open access available with fee

% ALENEX - August 11  submission, October 31 notification, https://www.siam.org/conferences/cm/submissions-and-deadlines/alenex22-submissions-deadlines, Alexandria, January 9-10
% SIGMOD - September 15th, feedback November 1-8, accept reject December 6th https://2022.sigmod.org/calls_papers_important_dates.shtml, Philly, June 12-17th
% VLDB -  February 28th, https://vldb.org/2022/ Sydney, September 5-9

% SODA - Passed, no 2023 date yet https://www.siam.org/conferences/cm/conference/soda22
% USENIX ATC - Carlsbad, CA, 2021 Passed, no 2022 website yet https://www.usenix.org/conferences/byname/131
% KDD - Passed, no 2022 website yet https://kdd.org/conferences
% ICDE - Passed, no 2023 website yet https://icde2022.ieeecomputer.my/research-track/
% SEA -Passed, no 2022 date yet

% PVLDB - same conference as VLDB, submit here first http://vldb.org/pvldb/vol15-submission/


% \documentclass[letterpaper]{article}
\documentclass[sigconf, nonacm]{acmart}
%\documentclass[manuscript,screen,review]{acmart}

\pdfoutput=1

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{graphicx}
%\PassOptionsToPackage{hyphens}{url}\usepackage[pdftitle={Stretching your data with taffy filters}]
%\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{microtype}
% TODO: fix underscores
%\usepackage[strings]{underscore}
%\usepackage{doi}
%\usepackage{nicefrac}
\usepackage{listings}
%\usepackage{todonotes}
\usepackage{epigraph}
%\usepackage{ifthen}
%% \usepackage{tikz}
%% \usetikzlibrary{arrows.meta}
%\usepackage[export]{adjustbox}
%\usepackage{framed}
\usepackage{float}

\newtheorem{theorem}{Theorem}

\lstset{
%    frame=tb, % draw a frame at the top and bottom of the code block
%    tabsize=2, % tab space width
%    showstringspaces=false, % don't mark spaces in strings
  numbers=left, % display line numbers on the left
%    commentstyle=\color{green}, % comment color
%    keywordstyle=\color{blue}, % keyword color
%    stringstyle=\color{red}, % string color
  basicstyle=\small\ttfamily,
  basewidth = {.48em},
  captionpos=b
}

\DeclareMathOperator{\adj}{adj}

%\renewcommand\UrlFont{\color{blue}\rmfamily}

%% \newcommand{\reals}{\mathbb{R}}
%% \newcommand{\rats}{\mathbb{Q}}
%% \newcommand{\nats}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
%% \newcommand{\cplx}{\mathbb{C}}
\newcommand{\defeq}{\;\genfrac{}{}{0pt}{2}{\text{def}}{=}\;}
\newcommand{\dotcup}{\ensuremath{\mathaccent\cdot\cup}}

%\pagestyle{empty}

% https://tex.stackexchange.com/questions/171803/change-font-size-of-the-verbatim-environment
% \newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}

%% \usepackage{etoolbox}
%% \makeatletter
%% \patchcmd{\@verbatim}
%%   {\verbatim@font}
%%   {\verbatim@font\small}
%%   {}{}
%% \makeatother


%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{https://github.com/jbapple/libfilter}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 


\begin{document}


\title{Stretching Your Data With Taffy Filters}
\author{Jim Apple}
\orcid{0000-0002-8685-9451}
\affiliation{%
  \city{Los Gatos, California}
%  \country{United States of America}
}
\email{jbapple@apache.org}

\begin{abstract}
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003752.10003809.10010055.10010056</concept_id>
       <concept_desc>Theory of computation~Bloom filters and hashing</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10002951.10002952.10002971.10003450.10010829</concept_id>
       <concept_desc>Information systems~Point lookups</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Point lookups}
\ccsdesc[500]{Theory of computation~Bloom filters and hashing}

\keywords{bloom filters, dictionaries, hash tables}

%\thispagestyle{empty}


\begin{abstract}
Popular approximate membership query structures such as Bloom filters and cuckoo filters are widely used in applications in databases, security, and networking.
These structures support two operations -- insert and lookup; lookup is always true on elements inserted into the structure, while it is true with some probability $\varepsilon \ll 1$ on elements {\em not} inserted into the structure.
Compensatory for these false positives, filters can be much smaller than hash tables that represent the same set.
However, unlike hash tables, cuckoo filters and Bloom filters must be initialized with the intended number of inserts to be performed, and cannot grow larger --
inserts beyond this number may fail or increase the false positive probability.
This paper presents designs and implementations of filters than can grow without inserts failing and without increasing the false positive probability, even if the filters are created with a small initial size.
The resulting code open source and available for general use.\footnote{\url{https://github.com/jbapple/libfilter}}
%\epigraph{If you can look into the seeds of time, and say which grain will grow and which will not, speak then unto me.}{Macbeth}
\end{abstract}

\maketitle
%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%


\section{Introduction}

Bloom filters are a ubiquitous data structure that allows storing a set with a low amount of space.
They support the operations insert, which adds an item to the set, and lookup, which returns true if an element is in the filter; if an element is not in the filter, true is returned with some configurable probability $\varepsilon \ll 1$.
This is called the ``false positive probability'', or ``fpp''

There are a number of other structures also supporting insert and lookup with a false positive probability greater than 0, including cuckoo filters, vacuum filters, Morton filters, ribbon filters, quotient filters, broom filters, and xor filters.~\cite{vacuum,morton-journal,ribbon,xor-filter,quotient-filter,broom}
A lookup operation with these guarantees is sometimes called an ``approximate membership query'', and structures that support approximate membership queries are sometimes referred to just as ``filters'' or ``AMQ structures''.
The significant interest in filters is reflective of their utility in applications such as databases, security, and networking.~\cite{split-bloom,vacuum,quotient-filter,malware,profile-similarity,invertible}

Each of the filter structures mentioned above supports approximate membership queries on sets with a given maximum size, but the question of extensible (or {\itshape extendable} or {\itshape incremental} or {\itshape growable}) filters that can increase in capacity as more elements are inserted is little studied
The classic answer is to create a sequence of Bloom filters, possibly of increasing sizes and/or lower false positive probabilities.~\cite{dynamic-bloom,scalable-bloom}
Inserts occur on the last filter to be created and lookups must search each filter.
Even when this keeps the false positive rate low, lookup times balloon from constant to logarithmic or even linear in $n$, the number of elements inserted.~\cite{psw,logarithm,consistent-cuckoo} %The Dynamic Cuckoo Filter
Additionally, the space usage can grow up to $\Omega(n \lg n)$, at which point a traditional hash table would do the same work in the same space with constant-time operations and a $n^{-c}$ false positive probability, where $c$ depends on the constant in $\Omega(n \lg n)$.
A newer approach to manage growing filters is to use cuckoo or quotient filters in which, when the filter grows, the false positive probability doubles.~\cite{logarithm,morton-journal,vacuum,rsqf}

Pagh et al. introduced a structure that performs lookups in $O(1)$ time while still allowing the structure to grow and keep a low false positive rate (not exceeding a threshold specified when the structure was created) all while using no more than $O(\lg \lg n + \lg (1/\varepsilon))$ words of space per element.~\cite{psw}
That work ends with an open problem of implementing these structures in a practical way.
This work answers that challenge with three new extensible filters:

\begin{enumerate}
\item Section~\ref{pbf} presents {\em taffy block filters} (``TBFs''), a practical implementation of the first AMQ structure from Pagh et al.~\cite{psw}.
\item Section~\ref{tcf} presents {\em taffy cuckoo filters} (``TCFs''), a practical implementation of the second construction from Pagh et al.
\item Section~\ref{mtcf} presents {\em minimal taffy cuckoo filters} (``MTCFs''), a variant of TCFs that reduce space usage.
\end{enumerate}

Section~\ref{eval} describes experimental performance results on all three taffy filters and what circumstances each is suited for.
Section~\ref{conclusion} concludes with some open questions.

% TODO: block filters with 16 hash functions - when useful?

\section{Prior work}

\subsection{Filters} 

\subsubsection{Split block Bloom filters}

\cite{overtakes,ultra-fast,split-bloom,block-bloom}

\subsubsection{Cuckoo filters}

\cite{cuckoo}

\subsection{Filters that can grow}

Pagh et al. describe two constructions to support extensible filters.~\cite{psw}
The first is implemented as a series of Bloom filters.
Common similar constructions use geometrically decreasing false positive probabilities in each subsequent filter in order to bound the total false positive rate.
That is, they create a sequence of Bloom filters with the following pairs for the false positive probability and expected number of distinct values:

\[
\langle \varepsilon / 2, 2 \rangle,
 \langle \varepsilon / 4, 4 \rangle,
 \langle \varepsilon / 8, 8 \rangle,
 \langle \varepsilon / 16, 16 \rangle,
 \ldots
\]

This leads to a storage footprint of more than $(\lg n + \lg (1/\epsilon)) / \ln 2$ bits per element and a query time of $\lg^2 n + \lg (n/\varepsilon)$.
Pagh et al. reduce the lookup cost to $\lg n$ by using a dictionary like Raman and Rao's that has $O(1)$ query time per filter.~\cite{psw,succinct}
They also reduce the space usage to $O(n \lg \lg n)$ by using the sequence $\langle \varepsilon / i^2,  2^i \rangle$, rather than  $\langle \varepsilon / 2^i,  2^i \rangle$.
See Figure~\ref{pagh-1-diagram}.

\begin{figure}
\includegraphics[width=\columnwidth]{pagh-1-diagram}
\caption{\label{pagh-1-diagram}
Pagh et al.'s first construction.
In this construction , $\lceil\lg (n-1) \rceil$ dictionaries are maintained with exponentially increasing capacities and logarithmically increasing bit widths.
The false positive probability of the $n$th dictionary, counting from $1$, is $6 \varepsilon / i^2 \pi^2$, and the sum of the false positive probabilities is $\le \varepsilon$.
The lookup operation requires a dictionary lookup in $\lceil\lg(n-1)\rceil$ dictionaries.\\
In this diagram, columns of blocks represent dictionaries.
Quotienting or other space factors are not visible, and neither are lookup or insert patterns.
}
\end{figure}

Pagh et al. also present a filter with the same space usage but $O(1)$ query time.~\cite{psw}
This filter maintains a map where the keys are bit strings of length $\lg n + \lg (1/\varepsilon)$ and the values are bit strings of length up to $\lg \lg N$, where $N$ will be the largest size of the data structure.
After every $2^i$ insertions, a new map is created where the keys are one bit longer.
The extra bit in the key is acquired by stealing a high order bit from the value.
This shrinks the value by one bit.
If there is nothing to be stolen because the value is the empty bit-string, two keys are generated, one with a new 0 appended, one with a 1; both are inserted into the map with associated value the empty bit-string.
See Figure~\ref{pagh-diagram}.

\begin{figure}
\includegraphics[width=\columnwidth]{pagh-diagram}
\caption{\label{pagh-diagram}
Pagh et al.'s second construction.
In this construction of a growable filter, when a filter contains $n$ items, it is stored as a dictionary in which the keys are bit stings of length $\lceil \lg n \rceil + \lg (1/\varepsilon) + 2$ and the values are bit strings of length up to $\lg \lg U$.
When $\lceil \lg n \rceil$ increases, the first bit of the value is moved to be a new last bit of the key, thus extending the key length.
Pagh et al. show that the fpp of such a dictionary is no more $\varepsilon$ as long as $n < U$.
}
\end{figure}

Taffy filters extend the work of Pagh et al. to support Bloom filters in the first construction and space-efficient cuckoo hashing in the second.
This work also implements the Pagh et al. structures for the first time and recommends circumstances under which each construction should be used.

\subsection{Succinct dictionaries}

Maps of size $n$ with keys from a universe of size $U$ and values from a universe of size $V$ can be naively stored in $n (\lg U + \lg V)$ bits by storing every element in an array (with any order) of size $n$.
Space can be saved using a technique called ``quotienting''.~\cite{knuth,quotient-filter}
The basic construction can be illustrated as follows:
first, an an array of size $n$ is created in which each array slot can hold an arbitrary number of kay-value pairs.
Then, a key-value pair $(k, v)$ is stored in slot $k \bmod n$.
Additionally, instead of storing $(k, v)$ explicitly, $(\lfloor k / n \rfloor, v)$ is stored; $\lfloor k / n \rfloor$ is the explicitly-stored part of the key, and $k \bmod n$ is the implicitly-stored part of they key.
%% The full value of $k$ can be reconstructed as $(k \bmod n) + n \lfloor k / n \rfloor$.
Because only $\lfloor k / n \rfloor$ is stored as the key, only $\lg U - \lfloor \lg n \rfloor$ bits are required to store it.
Coming back to the array, this reduces the total storage required to $n (\lg U - \lfloor \lg n \rfloor + \lg V)$.

This technique is used in quotient filters.~\cite{quotient-filter}
In quotient filters, the universe $V$ is empty and the universe $U$ has size $N/\varepsilon$, where $N$ is a bound on the number of elements that will be inserted.
Values in the universe $U$ are derived by hashing keys (from any universe) and taking the low-order $\lg n + \lg (1/\varepsilon)$ bits: if $h$ is a hash function and $x$ is a key, the key in $U$ to be stored is $h(x) \bmod n/\varepsilon$.

Quotienting can also be used with cuckoo hash tables.
Cuckoo hash tables maintain $k \ge 1$ locations for each key-value pair, which could be stored in any of them.~\cite{cuckoo-journal}
When a new element is inserted, if its locations are full, it evicts an existing element out of one of the locations; that evicted element must then be placed in one of its $k$ locations, and so on.
Because more than one hash function is used and because eviction occurs, it must be possible to translate from a location-element pair to an alternate location-element pair for the same key
%% Let $k$ be the number of locations and let $h_i$ for $0 \le i < k$ be the hash functions that map 

%% It thus

%% The quotienting reconstruction of the key described above does not fully translate to cuckoo hash tables: reconstruction requires a single slot number.
%% If reconstruction followed the pattern above, it would be possible to reconstruct two non-equal keys for the same element, just depending on which of its two or more slots the element was stored in.

Backyard cuckoo hashing handles this by using $k$ different hash functions.~\cite{backyard}
Upon insert, each new key-value pair is first hashed with the $k$ hash functions, then $k$ locations are identified via quotienting.
That is, an element $x$ maps to $h_i(x)$ for $i < k$, and then the slot $h_i(x) \bmod n$ is checked.
When an element is evicted, it is moved from $h_i(x)$ to some other $h_j(x)$, $j \ne i$.
In order to caculate $h_j(x)$, each hash function $h_i$ is a permutation with an inverse, so $h_j(x)$ can be calculated as $h_j(h_i^{-1}(h_i(x)))$.

TODO: use only a single permutation with forward and backward directions?



\subsection{Compact extensible dictionaries}

Hash tables used to accommodate sets without a size known in advance typically do so by doubling in capacity.
This means that at least 50\% of the space goes unused at points, with an average unused percentage of at least 25\%.
Constructions like that of Raman and Rao are able to mitigate this, but they are largely theoretical.~\cite{succinct}
Instead, Maier et al. uses the cuckoo hashing evict operation to incrementally resize a hash table.~\cite{dysect}
First, the ``DySECT'' table is broken up into equal sized sub-arrays that can be resized independently.
When the table gets close to full, exactly one of the sub-arrays is doubled in size.
This frees up room that's available in future eviction sequences, and the new space will slowly be filled.
Eventually all arrays will have been doubled in size, thereby causing the whole table to have doubled in size without going through a phase with as low as 50\% space usage.

Taffy filters extend quotienting-based dictionaries such as quotient filters and cuckoo filters to DySECT tables for the first time.

\section{Taffy block filters}
\label{pbf}

TODO: union, intersection, delete

The first construction from Pagh et al. consists of a set of sub-filters of decreasing false positive probabilities but exponentially increasing size (in terms of the number of distinct elements).~\cite{psw}
The $i$th sub-filter is initialized to have false positive probability $6 \varepsilon/(i^2 \pi^2)$ when filled to a capacity of $2^i$.
As Pagh et al. describe it, this filter is constructed with only the first sub-filter in place.
Inserts take place on the most recently added sub-filter, while lookups are performed by performing a lookup in each sub-filter until the element is found or there are no more sub-filters to search.
Once $2^i$ inserts have taken place, a new sub-filter is initialized and added to the collection.

Using traditional Bloom filters, the lookup cost would be

\[
\begin{array}{r c l}
\displaystyle\sum_{i=1}^{\lg n} \lg (i^2 \pi^2 /(6 \varepsilon)) & = &
 \displaystyle\sum_{i=1}^{\lg n} \lg (i^2) + \lg( \pi^2) - \lg 6 + \lg (1/\varepsilon) \\
& = & \Theta(\lg^2 n + \lg n \lg (1/\varepsilon))
\end{array}
\]

Instead, Pagh et al. use dictionary-based filters that support constant-time lookup -- such as cuckoo filters or Raman and Rao's dictionary -- rather than Bloom filters.~\cite{succinct,psw}

In place of a dictionary-based filter, taffy block filters use block Bloom filters with constant-time operations.~\cite{block-bloom,ultra-fast,cuckoo-filter-github}
A block Bloom filter is implemented as an array with an integral number of non-overlapping blocks.
Each block is itself a Bloom filter.

To insert a key, the key is first hashed once to select the block to use, mapping a key $x$ to $h(x) \bmod m/B$, where $h$ is the hash function, $m$ is the size of the block Bloom filter and $B$ is the size of each block.
Once a block is selected, it is used as a ``split'' Bloom filter.~\cite{split-bloom}
In a standard Bloom filter, to insert a key $x$, some number of hash functions $k$ are applied to $x$, and each bit $h_k(x) \bmod B$ is set.
In a split Bloom filter, the filter is split into $k$ equal-sized non-overlapping ``lanes'', each of size $L$.
Upon insertion, the bits $i L + (h_i(x) \bmod L)$ for $0 \le i < k$ are set.

In taffy block filters, a block Bloom filter is used with block size $B = 256$, $k = 8$ hash functions, and lane size $L = 32$~bits.
This enables using vector instructions to perform eight hash function computations at once using single-instruction-multiple-data instructions, i.e. vectorization.
%in what Polychroniou and Ross call ``horizontal vectorization''.~\cite{horizontal}
The resulting Bloom filter has constant-time branch-free insert and lookup and is consistently faster than a cuckoo filter of the same size (See Figure~\ref{lookup-both}).~\cite{cuckoo-filter-github}

%Setting a fixed $k$ and using variable false positive probabilities ensures that the size of sub-filter needed will exceed that of a traditional Bloom filter.
%The ideal number of hash functions is \lg (i^2\pi^2/6\varepsilon).


\begin{figure}
  \includegraphics[width=\columnwidth]{sbbf-diagram}
\caption{\label{sbbf-diagram}
A diagram of a split block Bloom filter.
To insert an element, a single bit is written in each of eight 32-bit lanes.
Those lanes are stored contiguously in a single block, and each block fits within a single cache line.
The bit to set in each lane is calculated by hashing the key eight times.
}
\end{figure}


%% TBFs cannot perform the union operation without significant increases in false positive probability.
%% For instance, in two split block Bloom filters of size $m$ where each level has $\delta m$ its bits set, the expected number of bits set in their union is $(2 \delta - \delta^2) m$.
%% Each of the input filters has false positive probability $\delta ^ 8$, while the output filter has false positive probability of nearly $2^8$ times that.

\section{Taffy cuckoo filters}
\label{tcf}

%% Taffy cuckoo filters (``TCFs'') are approximate membership query structures that offer the following operations:

%% \begin{itemize}
%% \item {\bf Create} initializes an empty filter with the given size.
%%   %and false positive probability, and expected {\em growth factor}.
%%   %A growth factor is the maximum number of doublings the structure must be able to undergo while maintaining the given false positive probability.
%% \item {\bf Insert} adds a key to the filter.
%%   This takes $O(1)$ expected time.
%%   It can fail when the filter is nearly full.
%% \item {\bf Lookup} takes a key and returns \verb|True| if the key is in the structure and \verb|False| with probability $1-\varepsilon$ if the key is not in the structure.
%%   It takes $O(1)$ worst-case time.
%% \item {\bf Upsize} doubles the capacity of the filter in $\Theta(n)$ time.
%% %% \item {\bf Union} adds the items from another filter of size $m$ in $O(m)$ time.
%% %% \item {\bf Intersection} Produces a new filter with only the items in both input filters, taking O(min(m,n)) time
%% \end{itemize}

Taffy block filters require $O(\lg n)$ lookup operations on their sub-filters.
Pagh et al.'s second construction requires only $O(1)$ time per lookup, and taffy cuckoo filters are a concrete implementation of that.

We use quotienting cuckoo tables (as in backyard cuckoo hashing~\cite{backyard}) to store the data in a taffy cuckoo filter.
The keys are bit-strings of length $\lg n + F$ and the values are bit-strings of length $V$, for some fixed $F$ (for ``fingerprint'') and $V$ (for ``value'').
By quotienting, each key-value pair can be stored in $\lg n + F + V - \lg n + O(1)$ bits, for a total space usage of $(F+V)n + O(n)$.
For performance and simplicity purposes, we pick $F + V = 16$, but this is not a requirement of the structure.
This matches the cuckoo filter implementation, which picks element size (in bits) as either 12 or a power of two.

%% The quotienting table uses permutation hash functions to transform the input key into two different hash values such that either can be used to reconstruct a portion of the original key.
%% The top $\lg n - O(1)$ bits are omitted from the stored hash values, since these are implicitly stored in the location of the key within the table.
%% In this way, data can be moved from one sub-table to the other via key reconstruction, even without storing the key explicitly.

TCFs are based on cuckoo filters and have two sides of slots, rather than one large pool of them.
TCFs also use bucketing for cuckoo hashing, in which slots are collected into groups called ``buckets''.~\cite{buckets}
Each side of a TCP comes equipped with a permutation on bit-strings of length $\lg n + F$.
A key-value pair $(x, y)$, where $x$ is a bit-string of length $\lg n + F$ and y is a bit-string of length $V$, is stored in one of two buckets: the one pointed to by the first $\lg n - O(1)$ bits of $P_0(x)$ or the one pointed to by the first $\lg n - O(1)$ bits of $P_1(x)$, where $P_i$ is the permutation associated with side $i$.

More concretely, an element consists of two groups of bits: one of size 10, and the other of size up to 5.
The former is called the fingerprint and the latter is called the tail.
A bucket consists of four (possibly empty) slots, each of which can hold one element.
A side consists of $2^k$ buckets for some $k$ as well as a permutation on $\ints_2^{k+10}$. %\footnote{$\ints_2^m$ and $\ints_{2^m}$.}
A TCF consists of two sides and one hash function that produces a 64-bit key.
The two sides have the same number of buckets but different permutations.
See Listing~\ref{tcf-types}.

\begin{lstlisting}[escapeinside={`}{`},label=tcf-types,
    caption={The types of a TCF.
      $S_i$ is the symmetric group on $\ints_i$.
      \texttt{T}$_\bot$ means the type \texttt{T} extended with the element $\bot$, indicating ``null'' or ``empty''.
      %As above, pseudocode will not make a distinction between $\ints_2^i$ and $\ints_{2^i}$.
      \texttt{T[n]} denotes an array of $n$ values of type $T$.
      $A \dotcup B$ represents a tagged disjoint union of $A$ and $B$; even if $A \subseteq B$, $A \dotcup B \ne B$.
      Structs are denoted by curly brackets \{\}, and members of structs can be referenced by their name or by their type.
      For instance, if \texttt{s} is a \texttt{Side(k)}, then \texttt{s.Bucket} and \texttt{s.Permutation} are both meaningful expressions.
    },
    float]
Element := {fingerprint: `$\ints_2^{10}$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`}
Slot := Element`$_\bot$`
Bucket := Slot[4]
Side(k) := {Bucket[2`$^k$`], Permutation: `$S_{2^{k+10}}$`}
TCF(U, k) := {Side(k)[2], HashFunction: `$U \to \ints_2^{64}$`}
\end{lstlisting}

A lookup begins by hashing a key with the TCF's hash function.
Then the lookup operation does the following:

\begin{enumerate}
\item Applies a permutation to the most-significant $k+10$ bits in the key. (Listing~\ref{tcf-lookup}, line 14)
\item Reserves the next 5 bits of the key; this will be the key's tail.
\item Using the most-significant $k$ bits in the permuted bits, selects a bucket. (Line 15)
(The remaining 10 bits in the permuted value are the fingerprint.)
\item Checks to see if any slot contains an element with an identical fingerprint. (Line 3)
If so, checks if the element's tail is a prefix of the key's tail.
If yes, returns \verb|True|.
Otherwise, returns \verb|False|.
\end{enumerate}

%TODO: endianness problems pervade the pseudocode?

\begin{lstlisting}[escapeinside={`}{`},
    float,
    label=tcf-lookup,
    caption={
      Pseudocode for the lookup operation on TCFs.
      \texttt{S[a,~b)} denotes the bits in $S$ starting at location $a$ and continuing through $b-1$.
    }]
LookupBucket(fingerprint, tail, bucket) {
  for (element : bucket) {
    if (element.fingerprint == fingerprint
        && element.tail IsPrefixOf tail) {
      return True
    }
  }
  return False
}

Lookup(input: U, tcf: TCF(U, k)) {
  hashed := tcf.HashFunction(input)
  for (side : tcf.Side[0], tcf.Side[1]) {
    permuted := side.Permutation(hashed[0, k+10))
    bucket := side.Bucket[permuted[0, k)]
    if LookupBucket(permuted[k, k+10), hashed[k+10, k+15),
                    bucket) {
      return True
    }
  }
  return False
}
\end{lstlisting}
%% \caption{\label{tcf-lookup}
%% Pseudocode for the lookup operation on TCFs.
%% \texttt{S[a,~b)} denotes the bits in $S$ starting at location $a$ and continuing through $b-1$.
%% }

%% \end{figure}

The insert operation first searches for the key, and, if found, returns (Listing~\ref{tcf-insert}, line 29).
Otherwise, insert places the key's fingerprint and tail in an empty slot, if one is found.
If none is found, insert selects an occupied slot from the bucket to {\em evict}: the element in this slot will be moved to the other side. (Listing~\ref{tcf-insert}, line 19)

The evict operation first reconstructs the high order $k + 10$ bits of the key by concatenating the $k$ bits of the bucket index and the $10$ bits of the fingerprint, then applying that side's permutation in reverse to the value. (Listing~\ref{tcf-insert}, line 24)
Using the same tail (this does not get permuted), the evict operation then inserts the evicted data into the opposite side using the insert algorithm described above.

\begin{lstlisting}[escapeinside={`}{`},
    label=tcf-insert,
    float,
    caption={
      Pseudocode for the insert operation on TCFs
  }]
InsertBucket(fingerprint, tail, bucket) {
  new_element := {fingerprint, tail}
  for(slot : bucket) {
    if (slot == `$\bot$`) {
      slot := new_element
      return `$\bot$`
    }
  }
  swap(new_element, RandomSlotIn(bucket))
  return new_element
}

InsertSide(tcf: TCF(U,k), side: `$\ints_2$`, hashed: `$\ints_2^{k+10}$`,
           tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
  permuted := side.Permutation(hashed)
  bucket := side.Bucket[permuted[0, k)]
  slot = InsertBucket(permuted[k, k+10), tail, bucket)
  if (slot == `$\bot$`) return
  Evict(tcf, side, permuted[0, k), slot)
}

Evict(tcf: TCF(U,k), side: `$\ints_2$`, bucket_index: `$\ints_{2^k}$`, element) {
  permuted := Concat(bucket_index, element.fingerprint)
  hashed := tcf[side].Permutation`$^\texttt{-1}$`(permuted)
  InsertSide(tcf, 1 - side, hashed, element.tail)
}

InsertTCF(input: U, tcf: TCF(U, k)) {
  if (Lookup(input, tcf)) return
  hashed := tcf.HashFunction(input)
  InsertSide(tcf, 0, hashed[0, k+10), hashed[k+10, k+15))
}
\end{lstlisting}

When a TCF is nearly full, inserts may fail.
This is identical to the situation with cuckoo filters.
When this happens, the \texttt{Upsize} method must be called to double the size of the structure.

The upsize operation begins by creating a new TCF.
To transfer the data from the older to the newer TCF, upsize uses a modified version of the evict algorithm.
Upsize first reconstructs the $k+10$ bits of the key that were used to construct the bucket index and fingerprint.(Listing~\ref{tcf-upsize}, line 14)
Then a bit is ``stolen'' from the tail and appended onto the end of the key.
Since the tail was taken unaltered from the key, this gives $k+11$ bits of the original key.
The new tail is now decreased in size by one.
The key and the new tail of it can now be inserted into one of the sides of the new TCF as described above. (Line 17)

This works as long as the tail has positive length.
If the tail has length zero, there is nothing to steal from.
Instead, two candidate keys are created from the reverse-permuted $k+10$ bits by appending a zero and a one.
It's indeterminate which one of these was in the original key, so both are inserted. (Line 20)
Pagh et al. show that the fpp is less than $2^{-8}$, although in TCFs the actual value will be smaller, since the tail is checked as well.~\cite{psw}

\begin{lstlisting}[escapeinside={`}{`},float,label=tcf-upsize,
    caption={Pseudocode for the upsize operation on TCFs.
  }]
Steal(head, tail) {
  if (|tail| > 0) {
    return Concat(head, tail[0]), tail[1, |tail|)
  }
  return Concat(head, 0), Concat(head, 1)
}

Upsize(tcf: TCF(U, k)) {
  result := new TCF(U, k + 1)
  for (side : tcf) {
    for (bucket_index : `$\ints_{2^k}$`) {
      for (element : side.Bucket[bucket_index]) {
        permuted := Concat(bucket_index, element.fingerprint)
        hashed := side.Permutation`$^\texttt{-1}$`(permuted)
        if (|tail| > 0) {
          (head, tail) := Steal(head, tail)
          InsertSide(result, 0, head, tail)
        } else {
          for (longhead : Steal(head, tail)) {
            InsertSide(result, 0, longhead, tail)
          }
        }
      }
    }
  }
  tcf = result
}
\end{lstlisting}

%% TCFs support a union operation as well, which operates by iterating over the smaller TCF and inserting into the larger.
%% The iteration is able to partially reconstruct the original hashed key (before permutations) using the same method as in upsize in Figure~\ref{tcf-upsize}, lines 10-14.
%% Although this does not allow the insert method to be used directly, a variation on insert (Figure~\ref{tcf-insert}, line 13-19) can insert a pair of a prefix of a hashed value and a tail.
%% Like upsize, union then steals bits from the tail, except union keeps stealing until the hash value is as long as needed to fit with the insert of the TCF being uniond into.
%% This results in a false positive probability that is the sum of the false positive probabilities of the two input TCFs.

%% TODO:  union benchmarks

%% TODO: union proofs

%% % TODO: intersection

%% \begin{figure}
%% \begin{lstlisting}[escapeinside={`}{`}]
%% InsertUnion(tcf: TCF(U,k), side: `$\ints_2$`, hashed: `$\ints_2^{j+10}$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
%%   if (j == k) return InsertSide(tcf, side, hashed, tail)
%%   if (|tail| > 0) {
%%     (head, tail) := Steal(hashed, tail)
%%     return InsertUnion(tcf, side, head, tail)
%%   }
%%   for (longhead : Steal(hashed, tail))
%%     InsertUnion(tcf, side, longhead, tail)
%%   }
%% }

%% Union(from: TCF(U, k), to: TCF(U, j)) {
%%   if (k > j) return Union(to, from)
%%   for (side : {0, 1}) {
%%     for (bucket_index : `$\ints_{2^k}$`) {
%%       for (element : from.Sides[side].Bucket[bucket_index]) {
%%         permuted := Concat(bucket_index, element.fingerprint)
%%         hashed := from[side].Permutation`$^\texttt{-1}$`(permuted);
%%         InsertUnion(to, side, hashed, element.tail)
%%       }
%%     }
%%   }
%% }
%% \end{lstlisting}
%% \caption{
%% Pseudocode for the union opeation on TCFs.
%% The two input TCFs must have the same hash function.
%% }
%% \end{figure}


% \subsection{Space and false positive analysis}



%% In~\cite{psw}, the dictionary is broken up into subsequences of size $2^i$.
%% Each subsequence $S_i$ contains the inserted keys with insertion order number in $[2^i, 2^{i+1})$.
%% Only when transitioning subsequences is the steal operation performed.
%% However, in PCFs, transitions happen when the dictionary is near full.
%% This does not necessarily correspond to the sequences.
%% Firstly, the $i$th dictionary cannot hold $2^i$ keys, just $\alpha 2^i$, where $\alpha$ si the ``fill factor''.
%% Second, the dictionary is sized according to the number of entries, while the structure in~\cite{psw} is sized according to the number of insertions.
%% These differ, as the number of entries for a sequence grows as the sequence lengthens as a result of the steals against an empty tail.

%% The false positive probability depends on the number of elements with different tail lengths.
%% Specifically, in a dictionary with $n$ elements where $p(i)$ is the number of elements with tail of length $i$, the false positive probability is less than

%% \[
%% 8 n^{-1} \sum_{i \le 7} 2^{-i-8}p(i)
%% \]

%% \[
%% 2^{-5} n^{-1} \sum_{i \le 7} 2^{-i}p(i)
%% \]

%% In any insertion sequence $I$, let $\alpha_I$ be the smallest number such that a dictionary with $\alpha_I x$ entries and capacity for $x$ entries is upsized.



%% \begin{theorem}
%%   $p(i)$
%% \end{theorem}~\begin{proof}
%% Assuming the dictionary starts with both sides having Bucket arrays of size 1, and thus having an element capacity of 8, the first $8 \alpha$ elements are inserted before any upsizing and therefore have tails of the full length, 7.
%% In this first phase, a candidate gets compared to at most $8 \alpha$ elements and has a $2^{-15}$ chance of collision with each, for a false positive probability (``fpp'') of less than $2^{-12}$.
%% Once the table has resized to having capacity 16, at most 8 of the slots are filles with tails of length 6.
%% The addition of at most 8 more items of length 7 makes

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 6\\
%% %% 8 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is less than $8 \cdot 16^{-1} (2^{-14}\cdot 8 + 2^{-15}\cdot 8) = 2^{-12} + 2^{-13}$.
%% %% Repeating, we get

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 5\\
%% %% 8 & i = 6\\
%% %% 16 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is less than $8 \cdot 32^{-1} (2^{-13}\cdot 8 + 2^{-14}\cdot 8 + 2^{-15}\cdot 16) = 2^{-12} + 2^{-13} + 2^{-13}$.

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 4\\
%% %% 8 & i = 5\\
%% %% 16 & i = 6\\
%% %% 32 & i = 7\\
%% %% 0 & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% and thus the fpp is $8 \cdot 64^{-1} (2^{-12}\cdot 8 + 2^{-13}\cdot 8 + 2^{-14}\cdot 16 + 2^{-15}\cdot 32) = 2^{-12} + 2^{-13} + 2^{-13} + 2^{-13}$.

%% %% This pattern repeats until and including

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 8 & i = 0\\
%% %% 2^{2+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% when $n = 2^{10}$ after inserting $2^10$.

%% %% so the fpp is less than $2^{-12} + 6 \cdot 2^{-13}$

%% %% After that, the next $p(i)$ is

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 3 \cdot 2^3 & i = 0\\
%% %% 2^{10} - 8 & i = 7 \\
%% %% 2^{3+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 4 \cdot 2^4 & i = 0\\
%% %% 2^{10} - 2\cdot2^2 & i = 6 \\
%% %% 2^{11} - 3\cdot 2^3 & i = 7 \\
%% %% 2^{4+i} & \text{otherwise}
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% 9 \cdot 2^9 & i = 0\\
%% %% 2^{10} - 2 \cdot 2^2 & i = 1 \\
%% %% 2^{11} - 3 \cdot 2^3 & i = 2 \\
%% %% 2^{12} - 4 \cdot 2^4 & i = 3 \\
%% %% 2^{13} - 5 \cdot 2^5 & i = 4 \\
%% %% 2^{14} - 6 \cdot 2^6 & i = 5 \\
%% %% 2^{15} - 7 \cdot 2^7 & i = 6 \\
%% %% 2^{16} - 8 \cdot 2^8 & i = 7 \\
%% %% \end{cases}
%% %% \]

%% %% \[
%% %% p(i) =
%% %% \begin{cases}
%% %% a  & i = 0\\
%% %% b & i = 1 \\
%% %% c & i = 2 \\
%% %% d & i = 3 \\
%% %% e & i = 4 \\
%% %% f & i = 5 \\
%% %% g & i = 6 \\
%% %% h & i = 7
%% %% \end{cases}
%% %% \]

%% $p$ with total size $m = 2^j$

%% changes to

%% \[
%% p(i) =
%% \begin{cases}
%% p(1) + 2p(0) & i = 0\\
%% p(2) & i = 1 \\
%% p(3) & i = 2 \\
%% p(4) & i = 3 \\
%% p(5) & i = 4 \\
%% p(6) & i = 5 \\
%% p(7) & i = 6 \\
%% 2^j - p(0) & i = 7 \\
%% \end{cases}
%% \]

%% with total size $2^{j+1}$.

%% The fpp changes from

%% \[
%% \varepsilon 8 \cdot 2^{-j} \left(\sum p(i) 2^{-i}\right)
%% \]

%% to

%% \[
%% 2^{-j + 2} \left(p(1) + 2p(0) + (2^j - p(0))2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% if $p(0) < 3 \cdot 2^{j-2}$, then this is at most

%% \[
%% 2^{-j + 2} \left(p(1) + 2 \cdot  3 \cdot 2^{j-2} + (2^j -  3 \cdot 2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% 2^{-j + 2} \left(p(1) + 3 \cdot 2^{j-1} + (2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% 2^{-j+2} \left(p(1) + 3 \cdot 2^{j-1} + (2^{j-2})2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i+1}\right)
%% \]

%% \[
%% \varepsilon (2^{-j+2}p(1) + 3 \cdot 2^{1} + 2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i-j+3}
%% \]

%% \[
%% \varepsilon (2^{-j+2}p(1) + 12 + 2^{-7} + \sum_{i = 1}^{i < 7} p(i) 2^{-i-j+3}
%% \]

%% Everything except 12 is vanishing, so 

%% \[
%% \begin{cases}
%% 2^{10} - 2 \cdot 2^2 & i = 1 \\
%% 2^{11} - 3 \cdot 2^3 & i = 2 \\
%% 2^{12} - 4 \cdot 2^4 & i = 3 \\
%% 2^{13} - 5 \cdot 2^5 & i = 4 \\
%% 2^{14} - 6 \cdot 2^6 & i = 5 \\
%% 2^{15} - 7 \cdot 2^7 & i = 6 \\
%% 2^{16} - 8 \cdot 2^8 & i = 7 \\
%% \end{cases}
%% \]


%% \[
%% p(i) =
%% \begin{cases}
%%  -2 \cdot 2^2 + 10 \cdot 2^{10} & i = 0\\
%% 2^{11} - 3 \cdot 2^3 & i = 1 \\
%% 2^{12} - 4 \cdot 2^4 & i = 2 \\
%% 2^{13} - 5 \cdot 2^5 & i = 3 \\
%% 2^{14} - 6 \cdot 2^6 & i = 4 \\
%% 2^{15} - 7 \cdot 2^7 & i = 5 \\
%% 2^{16} - 8 \cdot 2^8 & i = 6 \\
%% 2^{17} - 9 \cdot 2^9 & i = 7 \\
%% \end{cases}
%% \]

%% \[
%% p(i) =
%% \begin{cases}
%% - 5 \cdot 2^3 + 11 \cdot 2^{11} & i = 0\\
%% 2^{12} - 4 \cdot 2^4 & i = 1 \\
%% 2^{13} - 5 \cdot 2^5 & i = 2 \\
%% 2^{14} - 6 \cdot 2^6 & i = 3 \\
%% 2^{15} - 7 \cdot 2^7 & i = 4 \\
%% 2^{16} - 8 \cdot 2^8 & i = 5 \\
%% 2^{17} - 9 \cdot 2^9 & i = 6 \\
%% 2^{17} - 9 \cdot 2^9 & i = 6 \\
%% \end{cases}
%% \]

%% with $n = 2^{11} - 2^4 + 3 \cdot ^3$ after inserting $2^11$

%% and then

%% \[
%% p(i) =
%% \begin{cases}
%% 4 \cdot 2^4 & i = 0\\
%% 2^{4+i} & \text{otherwise}
%% \end{cases}
%% \]

%% with $n = 2^{12} - 2^5 + 4 \cdot 2^4$


%% following the pattern

%% \[
%% p(i) =
%% \begin{cases}
%% j \cdot 2^j & i = 0\\
%% 2^{j+i} & \text{otherwise}
%% \end{cases}
%% \]

%% when $n = j \cdot 2^j - 2^{j+1} + 2^{8+j}$ after inserting

%% In this case, the false positive probability is 

%% \[
%% 2^{-5} \cdot (j \cdot 2^j - 2^{j+1} + 2^{8+j})^{-1}(j \cdot 2^j + 2^{-1}2^{j+1} + 2^{-2}2^{j+2} \dots)
%% \]

%% \[
%% 2^{-5} \cdot (j \cdot 2^j - 2^{j+1} + 2^{8+j})^{-1}(j \cdot 2^j + 7\cdot2^j)
%% \]

%% \[
%% 2^{-5} \cdot (2^j(j - 2 + 2^8))^{-1}2^j(j + 7)
%% \]

%% \[
%% 2^{-5} (j - 2 + 2^8)^{-1}(j + 7)
%% \]

%% \[
%% \frac{j+7}{2^5 (j - 2 + 2^8)}
%% \]

%% \end{proof}

%% That is only approximated in PCFs, as the 

%% The calculations of how many slots the accumulation of these doublings produces is the key calculation in the space consumption analysis in~\cite{psw}.
%% That is crucial as well for the false positive rate
%% For the concrete setting here, if the dictionary starts with $k = 0$ (each side having size 1 bucket), then after $m$ upsizes, if $m < 7$, $2^{m+2}(1 - \delta)$ keys have been inserted and the same number are in the dictionary.
%% Half of the tails have length seven, a quarter have length six, and so on.
%% The false positive probability is $2^{m+2}(1 - \delta) \cdot 2^{-m-8}(m \cdot 2^{-8}) = m(1-\delta)2^{-14}$, which is between $6.1E-5$ and $4.2E-4$.
%% TODO: experimentally different. Make graph.
%% %% check to see if the tail in that slot is a prefix of the 

%% %% To look up a key

%% %% Plastic cuckoo filters are based on this filter, but additionally add a value to each key.
%% %% The value is the next $\lg d$ bits of the key, where $d$ is the growth factor.



%% %% Plastic cuckoo filters are based on a type of bloomier filter.
%% %% A bloomier filter is an AMQ supporting lookups, not just membership queries.
%% %% Given a domain and range $D$ and $R$ where $D \subset U$ and $M$ is a map from $D$ to $R$, a query for $x$ in a bloomier filter for $M$ returns $M(x)$ if $x \in D$ and $\bot$ with probability $1-\varepsilon$ when $x \in U \backslash D$, and otherwise returns an arbitrary value in $R$.

%% %% Next we will form a quotienting cuckoo map.
%% %% A quotienting cuckoo map has a domain of $\ints_d$ and a range of $\ints_r$.
%% %% A standard cuckoo map would use $d + r$ bits per unit of capacity.
%% %% A quotienting cuckoo table uses $d + r + \lg b -  \lg m $ where $m$ is the capacity of the table and $b$ is the size of the bucket.


%% %% In an elastic filter, the permuted key has length $\lg \nicefrac{1}{\varepsilon} + \lg m - \lg b$ while the range consists of all sequences of bits of length less than $\lg\lg \nicefrac{U}{n} + O(1)$.
%% %% When the table is nearly full, meaning that further additions would likely fail, the size is doubled.
%% %% This increases the size of the unstored bits by 1, so the number of stored bits would be smaller, as well.
%% %% Instead of letting that happen, a bit is ``stolen'' from the range of sequences of bits.

%% %% This shortens the value in the range, but the range stays the same.
%% %% If there are no bits to steal becuase the value is the empty string, {\em two} new values are generated to insert into the new, larger table: one with a one appended to the stored bits, one with a zero.
%% %% This follows directly from~\cite{psw}.

\section{Minimal taffy cuckoo filters}
\label{mtcf}

The previous structure suffers from a step-function space usage:
at each point, the structure has a size which is a power of two, sometimes allocating much more space than will be needed.
To address this, this section describes a similar structure using DySECT to reduce the space usage to only what is needed.~\cite{dysect}

DySECT is a variant of cuckoo hashing.
Just as in cuckoo hashing, upon an insertion, an element may be evicted.
A DySECT table consists of some number of {\em subtables}, and as the table gets more and more full, it grows by doubling the size of one of its subtables.
As new elements are inserted into the table, they evict older elements, and this movement causes the newly-doubled subtable to fill up.

Minimal taffy cuckoo filters (``MTCFs'') applies this idea to TCFs.
Some complications arise:

\begin{enumerate}
  \item Because subtables have different sizes, the bits that are implicitly stored using quotienting varies depends on which part of the table an element is in.
    To address this, fingerprints in MTCFs have variable size.
  \item Because fingerprints have variable size, there must be two permutations per side, depending on the length of the key to permute.
  \item Because there are two permutations per side, a key may be in one of up to four distinct buckets, which decreases the performance and increases the false positive probability.
\end{enumerate}

In an MTCF, each element has a fingerprint of size 8 or 9 and a tail of size up to 5.
A bucket consists of four (possibly empty) slots, each of which can hold one element.
A level consists of two tables of the same size, each with $2^k$ buckets for some $k$.
The table consists of four permutations, one hash function, $32$ levels, and one cursor pointing to some index in the set of levels.
The maximum and minimum $k$ across all levels differ by at most 1.
Levels at location less than the cursor have the larger size.
If all levels have the same size, the cursor must be 0.
See Listing~\ref{mtcf-types} and Figure~\ref{mtcf-diagram}.

The permutations are grouped by side, two for each.
The permutations are on values with length $k + 13$ and $k + 14$, where $2^k$ is the size of the smallest table, measured in buckets.

\begin{lstlisting}[escapeinside={`}{`},float,label=mtcf-types,
    caption={
      The types of an MTCF.
      ``$\times$'' means Cartesian product.
  }]
MElement := {fingerprint: `$\ints_2^8 \dotcup \ints_2^9$`, tail: `$\dotcup_{i \le 5} \ints_2^i$`}
MSlot := MElement`$_\bot$`
MBucket := MSlot[4]
Level(k) := MBucket[2][2`$^k$`] `$\dotcup$` MBucket[2][2`$^{k+1}$`]
Permutation(k) := `$S_{2^{k+13}} \times S_{2^{k+14}}$`
MTCF(U, k) := {cursor: `$\ints_{32}$`,
               Level(k)[32],
               Permutation(k)[2],
               HashFunction: `$U \to \ints_2^{64}$`}

\end{lstlisting}

\begin{figure}
  \includegraphics[width=\columnwidth]{mpcf-diagram}
\caption{\label{mtcf-diagram}
A diagram of an MTCF.
In this example, $k = 2$, so the larger levels have $2^{k+1} = 8$ buckets on each side, while the smaller levels have four.
In the first bucket of side 0 of the 32nd level, the last three slots are empty.
If the entry were from one of the larger levels above the cursor, then the fingerprint would have to be from $\ints_2^8$.
}

\end{figure}

If there are larger and smaller levels, then every element in the larger levels has a fingerprint of size 8, not 9.
This is because the implicitly-stored part of the key is one-bit longer in the larger levels.

In an MTCF, upsize only increases the size of one of the levels, not the whole structure.
As a result, the capacity of the filter tracks more closely the number of entries in the table.

A lookup operation in an MTCF first applies each of the four permutations to the hashed key.
For the permutations on $k + 14$ bits, the first five bits indicate the level, the next $k$ or $k+1$ indicate the bucket, and the remaining 8 or 9 bits are the fingerprint.
Lookup proceeds as it does in the TCF case, by checking if fingerprints match and if the stored tail is a prefix of the tail of the key being looked up.
For the permutations on $k + 13$ bits, the first five bits again indicate the level.
If the level has tables with $2^{k+1}$ buckets, the permuted key is not used for lookup; to do otherwise would leave only $k+13 - 5 - (k+1) = 7$ bits for the fingerprint, which is not possible.
That key is simply skipped and the lookup continues with the next key. %See Figure~\ref{mtcf-insert}.
Otherwise, the level has tables with $2^k$ buckets, and we can proceed as in the $k+14$ case.

%% Upsize only increases the size of one level.
%% After 31 upsizes, the next one makes all of the levels the same size.
%% At that point, two new permutations must be initialized to handle new longer key prefixes.

During an evict operation, an element may move between levels with differently-sized arrays of buckets.
This poses no problem when the fingerprint has size 9, but when the fingerprint has size 8 and the level moved {\em from} has a bucket array of size $2^k$ and the level moved {\em to} has a bucket array of size $2^{k+1}$, there are not enough bits in the fingerprint.
As above, we steal bits from the tail, and, as above, if there are no bits to steal, two new key prefixes are created and inserted, as one of them must be the prefix of the original key.
See Figure~\ref{mtcf-state-transition}

\begin{figure}
  \includegraphics[width=\columnwidth]{mtcf-state-transition}
\caption{\label{mtcf-state-transition}
This diagram illustrates the transitions an element can go through when evicted.
The diagram can be read as ``an element in state A can become an element in state B when evicted''.
The states indicate the lengths of the level, fingerprint, and tail.
When a level's index is less than the cursor, its length is twice what it would be if its index were higher.
In any of the four states, an eviction may move an element from that state to itself.
For instance, an element in a long level can be evicted to stay in a long level, and the same can be said of an element with a long fingerprint (which must necessarily be in a short level).
For elements in a short level with a short fingerprint and no tail, when they are evicted to an element in a long level, two elements are created, as it is impossible to steal a bit from the empty tail.
}
\end{figure}

Pseudocode for lookup, insert, and upsize are available in Appendix~\ref{mtcf-appendix}.


\section{Evaluation}
\label{eval}

TODO: evaluate starting taffy filters with more starting elements: maybe 1024

In each graph, TBFs are configured for a maximum fpp of 0.1\%.
%In case the insert operation fails, it calls Upsize until a place can be found for the element.
All taffy filters are configured with an initial capacity of 1, however the MTCF has a minimum size of 2 bytes/entry $\cdot$ 32 levels $\cdot$ 2 sides/level $\cdot$ 1 entry / side = 128 bytes.
All experiments were performed on an Intel i7-7800X with 96GB of memory and SMT turned on, except where noted

For performance testing, we equip both TCFs and MTCFs with variable sized stashes.~\cite{stash}
We set both filters to upsize when they are 90\% full or their stashes have size greater than 4.

For comparison, the graphs also include a cuckoo filter and a split-block bloom filter, each sized to hold 100 million elements with an fpp of 0.1\%.

\subsection{Space}

A filter with a false positive probability of $\varepsilon$ must take up at least $\lg (1/\varepsilon)$ bits per element.
Practical filters use more space.
For instance, Bloom filters use $\lg (1/\varepsilon)/\ln 2$ bits per element, which is about $1.44 \lg (1/\varepsilon)$.
Cuckoo filters and quotient filters use $(\lg (1/\varepsilon) + d) / \alpha$ where $d$ is between 2 and 3, and $\alpha$ is the fill factor, between 80\% - 99\%.~\cite{cuckoo,quotient-filter,vector-quotient}
Static filters that do not support insert -- such as the ribbon filter -- can use nearly optimal space.~\cite{ribbon}

However, Pagh et al. showed that filters that can grow, like taffy filters can, must use at least $\lg (1/\varepsilon) + \Omega(\lg \lg n)$ bits per element.~\cite{psw}
Figures~\ref{bits-per-item} and~\ref{ideal-bits-per-item} show the actual bits per element and $\lg (1/\varepsilon)$, respectively.

Cuckoo filters cannot grow (without changing the number of bits per slot and doubling the false positive rate.
As such, cuckoo filters are only visible over 10 million elements on the space graph, since their bits per key starts out at over 1.2 billion bits per.

% TODO: graph different MTCF variants based on level size, fingerprint size

% TODO: graph fill factor

% TODO: freeze/thaw operations with ribbon filters

% TODO: compare standard bloom filters, block bloom filters, filters with exponentially decreasing fpp, and cuckoo filters


\begin{figure}

  \includegraphics[width=\columnwidth]
{bits-per-item}
  \caption{  \label{bits-per-item}
Number of bits (per element) each filter type needs.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]
{ideal-bits-per-item}
  \caption{  \label{ideal-bits-per-item}
This figure shows $\varepsilon$, the false positive probability.}
\end{figure}


%% \begin{figure}
%%   \includegraphics[width=\textwidth]{deficiency}
%%   \caption{
%%     There is a lower bound on the amount of space that must be used to form an approximate membership query structure with a given false positive rate: $\lg \nicefrac{1}{\varepsilon}$ bits per element.
%%     Any additional space is, in a sense, ``wasted''.
%%     In another sense, this overstates the waste, as the best dynamic structures, like Bloom filters and semi-sorted cuckoo filters, do not achieve the lower bound.
%%     Lower is better.
%%   }
%% 
%% \end{figure}

%% \begin{figure}
%%   \includegraphics[width=\textwidth]{insert-deficiency}
%%   \caption{
%%     Comparing both the wasted space of each filter type with the absent lookup performance.
%%     Lower left is better.
%%   }
%% \end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]
{space}
  \caption{
    The amount of space used by each filter at the given number of keys inserted.
  }
\end{figure}

\subsection{Time}

Figures~\ref{insert-time}~and~\ref{lookup-both} show the performance of the taffy filters.
TBFs are the fastest, with the exception of absent lookups for filters with over a million elements, where TCFs and cuckoo filters are faster, needing only two cache misses to check for a key, while TBFs need to check 27 block filters.


\begin{figure}
  \includegraphics
[width=\columnwidth]
{insert}
  \caption{
    \label{insert-time}
    Insert times for filters.
    %% TODO: the dip is an artifact; redo with more samples and longer time between timer start \& stop.
  }
\end{figure}

\begin{figure}
  \includegraphics
[width=\columnwidth]
{lookup-both}
  \caption{
    \label{lookup-both}
    Lookup times for filters, i7-7800X
  }
\end{figure}

\begin{figure}
  \includegraphics
[width=\columnwidth]
{arm-lookup-both}
  \caption{
    \label{arm-lookup-both}
    Lookup times for filters on an AWS S3 m6g.medium with an ARM-based Graviton2.
  }
\end{figure}

%% \begin{figure}
%%   \includegraphics
%% [width=\columnwidth]
%% {lookup-absent}
%%   \caption{
%%     \label{lookup-absent}
%%     Lookup times for filters when the key is absent.
%%   }
%% \end{figure}

\subsection{Discussion}

The MTCF offers lower space than the other two taffy filters, but its performance is substantially worse.
It has significant insertion time increases when it is hard to find an eviction sequence; in this case consecutive insert operations may call Upsize, causing a spike in the graph.
This cyclic behavior was noted by Maier et al.~\cite{dysect}
%This behavior is also seen in the TCF, though less severely.
% Performance for lookup is also cyclic as the fill factor and size of the stashes grow and shrink.
Additionally, when the cursor is close to 32, the performance of lookup improves as the four potential locations to look for a key are more frequently reduced to two, since the shorter permuted keys are no longer long enough for most of the levels in the structure.

TBFs, which have an $\Theta(\lg n)$ lookup cost, come out ahead of the other types of taffy filters in both insert and lookup costs except when over a million elements are stored.
Split-block Bloom filters and cuckoo filters are still attractive choices when the size of the set to be approximated is known in advance.



% TODO: show different level counts and how that decreases the spikiness?

% TODO: what is the fpp of the dictionary, analytically

% TODO: explain permutation swapping

% TODO: explain combining tails

% TODO: show that longer fingerprints lead to higher fill factors

% TODO: Show how much time is spent in Upsize

% TODO: show how stash limits lead to higher insert times and lower lookup times

% TODO: show that the size of the dictionary stays limited, as in~\cite{psw}

\section{Future work}
\label{conclusion}

The stash limitations for the TCF and MTCF are very low - tuned for a linear search.
It is not necessary they be unstructured - giving structure to the stash, as in backyard cuckoo hashing, could allow the stash to grow and the fill factor to go higher without needing to upsize.~\cite{backyard}

There is a fourth structure supporting sets of unknown sizes described in~\cite{unknown-prefix}.
This structure is mostly theoretical and depends on a certain type of prefix search structure with large embedded constants.
A practical version of this may be possible to build.

For the TCF, alternate quotienting dictionaries, including quotient filters, broom filters, or bucketing hash tables (as presented by K\"oppl et al.) could potentially improve performance.~\cite{raman-practical,broom,quotient-filter}

Finally, testing taffy filters in various applications requiring the use of extensible approximate membership query structure would be of interest.

\begin{acks}
Thanks to Pedro Vasallo and Alex Breslow for helpful discussions and feedback.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{taffy}
\pagebreak
\appendix
\section{MTCF pseudocode}
\label{mtcf-appendix}
 
%% 1
%\begin{figure}%[!htbp]
\begin{lstlisting}[escapeinside={`}{`},caption={The lookup operation in an MTCF. Notice that up to four buckets may be searched: between one and two per side.}]
LookupSide(mtcf: MTCF(U, k), side: `$\ints_2$`, hashed: `$\ints_2^{64}$`) {
  permuted := mtcf.Permutation[side](hashed[0, k+13))
  level_number := permuted[0, 5)
  if (level_number `$\ge$` mtcf.cursor) {
    bucket := mtcf.Level[level_number][permuted[5, k + 5)]
    if (LookupBucket(permuted[k+5, k+13),
                     hashed[k+13, k+18), bucket)) {
      return True
    }
  }
  permuted = mtcf.Permutation[side](hashed[0, k+14))
  level_number = permuted[0, 5)
  middle := (level_number `$\ge$` mtcf.cursor) then k+5 else k+6
  bucket := mtcf.Level[level_number][permuted[5, middle)]
  return LookupBucket(permuted[middle, k+14),
                      hashed[k+14, k+19), bucket)
}

MLookup(mtcf: TCF(U, k), key: U) {
  hashed := mtcf.HashFunction(key)
  for (side : {0, 1}) {
    if (LookupSide(mtcf, side, hashed)) return True
  }
  return False
}
\end{lstlisting}
%\caption{The lookup operation in an MTCF.}

%\end{figure}
%% 2
%% \begin{figure}

\begin{lstlisting}[escapeinside={`}{`},caption={Pseudocode for the insert operation on MTCFs}]
MInsertSide(mtcf: MTCF(U, k), side: `$\ints_{2}$`, level_number: `$\ints_{32}$`,
            bucket_number: `$\ints_{2^k} \dotcup \ints_{2^{k+1}}$`, fingerprint `$\ints_2^8 \dotcup \ints_2^9$`,
            tail: `$\sum_{i \le 5}\ints_2^i$`,) {
  bucket := mtcf.Level[level_number][side][bucket_number]
  new_slot := InsertBucket(fingerprint, tail, bucket)
  if (new_slot == `$\bot$`) return
  fingerprint = new_slot.fingerprint
  tail = new_slot.tail
  permuted := Concat(level_number, bucket_number, fingerprint)
  hashed := mtcf.Permutation[side]`$^\texttt{-1}$`(permuted)
  permuted = mtcf.Permutation[1 - side](hashed)
  MInsertPermuted(mtcf, side, permuted, tail)
}

MInsertPermuted(mtcf: MTCF(U, k), side: `$\ints_2$`,
                permuted: `$\ints_2^{k+13} \dotcup \ints_2^{k+14}$`,
                tail: `$\sum_{i \le 5}\ints_2^i$`) {
  level_number := permuted[0, 5)
  if (|permuted| == k + 14) {
    middle := if (level_number `$\ge$` mtcf.cursor)
              then k+5 else k+6
    MInsertSide(mtcf, 1 - side, level_number,
                permuted[5, middle),
                permuted[middle, k+14), tail)
    return
  }
  if (level_number `$\ge$` mtcf.cursor) {
    permuteds := {permuted}
    middle := k + 5
  } else (|tail| > 0) {
    (permuted, tail) := Steal(permuted, tail)
    permuteds := {permuted}
    middle := k + 6
  } else {
    permuteds := Steal(permuted, tail)
    middle := k + 6
  }
  for (p : permuteds) {
    MInsertSide(mtcf, 1 - side, level_number, p[5, middle),
                p[middle, middle + 8), tail)
  }
}
\end{lstlisting}

\begin{lstlisting}[escapeinside={`}{`},caption={Pseudocode for the insert operation on MTCFs}]
InsertMTCF(mtcf: MTCF(U, k), key: U) {
  if (MLookup(mtcf, key)) return
  hashed := mtcf.HashFunction(key)
  permuted := mtcf.Permutation[0](hashed[0, k+14))
  level_number := permuted[0, 5)
  middle := if (level_number `$\ge$` mtcf.cursor)
            then k+5 else k+6
  bucket_number := permuted[5, middle)
  fingerprint := permuted[middle, k+14)
  tail := hashed[k+14, k+19)
  MInsertSide(mtcf, 0, level_number,  bucket_number,
              fingerprint, tail)
}
\end{lstlisting}
%% \caption{Pseudocode 2 for the insert operation on MTCFs}
%% \end{figure}

%% 5

%% \begin{figure}
\begin{lstlisting}[escapeinside={`}{`},caption={Pseudocode for the upsize operation on MTCFs}]
UpsizeMTCF(mtcf: MTCF(U, k)) {
  old := mtcf.Level[mtcf.cursor]
  mtcf.Level[mtcf.cursor] = new Level(2`$^{{k+1}}$`)
  mtcf.cursor = mtcf.cursor + 1
  for (side : {0, 1}) {
    for (bucket_index : `$\ints_{2^k}$`) {
      for (element : old.MBucket[side][bucket_index]) {
        cursor = cursor - 1
        permuted := Concat(cursor, bucket_index,
                           slot.fingerprint)
        hashed := mtcf.Permutation[side]`$^{\texttt{-1}}$`(permuted)
        cursor = cursor + 1
        permuted = mtcf.Permutation[side](hashed)
        MInsertPermuted(permuted, element.tail, side, mtcf)
      }
    }
  }
}
\end{lstlisting}
%% \caption{Pseudocode for the upsize operation on MTCFs}
%% \end{figure}

%% \begin{figure}
%% \begin{lstlisting}[escapeinside={`}{`}]
%% MInsertUnion(mtcf: MTCF(U, k), side: `$\ints_2$`, hashed: `$\ints_2^{j+13}\dotcup\ints_2^{j+14}$`,
%%              tail: `$\dotcup_{i \le 5} \ints_2^i$`) {
%%   if (j == k) {
%%     permuted = mtcf.Permutation[side](hashed)
%%     return MInsertPermuted(mtcf, side, permuted, tail)
%%   }
%%   if (|tail| > 0) {
%%     (head, tail) := Steal(hashed, tail)
%%     return MInsertUnion(mtcf, side, head, tail)
%%   }
%%   for (h : Steal(hashed, tail)) {
%%     MInsertUnion(mtcf, side, h, tail)
%%   }
%% }

%% Union(from: MTCF(U, k), to: MTCF(U, j)) {
%%   if (k > j) return Union(to, from)
%%   for (level : `$\ints_32$`)
%%     for (side : {0, 1}) {
%%       for (bucket_index : `$\ints_{2^k}$`) {
%%         for (element : from.Sides[side].Bucket[bucket_index]) {
%%           permuted := Concat(level, bucket_index, element.fingerprint)
%%           hashed := from[side].Permutation`$^\texttt{-1}$`(permuted);
%%           MInsertUnion(to, side, hashed, element.tail)
%%         }
%%       }
%%     }
%%   }
%% }

%% \end{lstlisting}
%% \caption{Pseudocode for the union operation on MTCFs.
%% The input MTCFs must have the same hash function.}
%% \end{figure}

\end{document}

%%  LocalWords:  growable lookups quotienting AMQ Upsize TCF TCF's
%%  LocalWords:  HashFunction LookupBucket IsPrefixOf tcf InsertTCF
%%  LocalWords:  InsertSide InsertBucket RandomSlotIn otherSide TCFs
%%  LocalWords:  Concat upsize doublings DySECT MTCF MSlot MBucket
%%  LocalWords:  upsizes TODO fpp LookupSide mtcf MLookup MInsertSide
%%  LocalWords:  MInsertPermuted permuteds MTCFs InsertMTCF MElement
%%  LocalWords:  UpsizeMTCF TBFs Structs structs SMT Pagh Segev FOCS
%%  LocalWords:  Wieder longhead TBF onn Raman Rao's Rao Maier Gatos
%%  LocalWords:  Vasallo Breslow PVLDB VLDB ISSN logarithmically
%%  LocalWords:  Polychroniou vectorization
